{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/a98zhang/PlayWithData/blob/master/CS224U_hw_sentiment.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZzTQZ_itJ8hK"
      },
      "source": [
        "# Homework and bakeoff: Multi-domain sentiment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i6ZSGIKx8TZT"
      },
      "outputs": [],
      "source": [
        "__author__ = \"Christopher Potts\"\n",
        "__version__ = \"CS224u, Stanford, Spring 2023\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2U_4KPNB8TZU"
      },
      "source": [
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/cgpotts/cs224u/blob/main/hw_sentiment.ipynb)\n",
        "[![Open in SageMaker Studio Lab](https://studiolab.sagemaker.aws/studiolab.svg)](https://studiolab.sagemaker.aws/import/github/cgpotts/cs224u/blob/main/hw_sentiment.ipynb)\n",
        "\n",
        "If Colab is opened with this badge, please **save a copy to drive** (from the File menu) before running the notebook."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LZVDgi2Y8TZU"
      },
      "source": [
        "## Overview"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cIduMdmb8TZU"
      },
      "source": [
        "This homework and associated bakeoff are devoted to supervised sentiment analysis in a ternary label setting (positive, negative, neutral). Your ultimate goal is to develop systems that can make accurate predictions in multiple domains."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xQ2HPZdq8TZU"
      },
      "source": [
        "The homework questions ask you to implement some baseline systems using DynaSent Round 1, DynaSent Round 2, and the Stanford Sentiment Treebank. The bakeoff challenge is to define a system that does well on the DynaSent test sets, the SST-3 test set, and a set of mystery examples that don't correspond to the DynaSent or SST-3 domains."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fOJ75gKv8TZV"
      },
      "source": [
        "__Important methodological note:__ The DynaSent and SST-3 test sets are already publicly distributed, so we are counting on people not to cheat by developing their models on these test sets. You must do all your development without using these test sets at all, and then evaluate exactly once on the test set and turn in the results, with no further system tuning or additional runs. _Much of the scientific integrity of our field depends on people adhering to this honor code._"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KWDcNBiK8TZV"
      },
      "source": [
        "This notebook briefly introduces our three development datasets, states the homework questions, and then provides guidance on the original system and associated bakeoff entry."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gsLcqWtBJ8hM"
      },
      "source": [
        "## Set-up"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2nFUcNIhJ8hM",
        "outputId": "0417554a-82bc-4fed-9fbd-c89e3d307c75"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cloning into 'cs224u'...\n",
            "remote: Enumerating objects: 2209, done.\u001b[K\n",
            "remote: Counting objects: 100% (117/117), done.\u001b[K\n",
            "remote: Compressing objects: 100% (78/78), done.\u001b[K\n",
            "remote: Total 2209 (delta 51), reused 62 (delta 39), pack-reused 2092\u001b[K\n",
            "Receiving objects: 100% (2209/2209), 41.48 MiB | 17.33 MiB/s, done.\n",
            "Resolving deltas: 100% (1350/1350), done.\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting git+https://github.com/stanfordnlp/dsp (from -r cs224u/requirements.txt (line 15))\n",
            "  Cloning https://github.com/stanfordnlp/dsp to /tmp/pip-req-build-3yo91ewt\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/stanfordnlp/dsp /tmp/pip-req-build-3yo91ewt\n",
            "  Resolved https://github.com/stanfordnlp/dsp to commit 6e179b4a5a9e47aa67047a1ea3555091cf99ec3c\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy>=1.20.0 in /usr/local/lib/python3.9/dist-packages (from -r cs224u/requirements.txt (line 1)) (1.22.4)\n",
            "Requirement already satisfied: scipy>=1.7.0 in /usr/local/lib/python3.9/dist-packages (from -r cs224u/requirements.txt (line 2)) (1.10.1)\n",
            "Requirement already satisfied: matplotlib>=3.7.0 in /usr/local/lib/python3.9/dist-packages (from -r cs224u/requirements.txt (line 3)) (3.7.1)\n",
            "Requirement already satisfied: scikit-learn>=1.0.2 in /usr/local/lib/python3.9/dist-packages (from -r cs224u/requirements.txt (line 4)) (1.2.2)\n",
            "Requirement already satisfied: nltk>=3.7 in /usr/local/lib/python3.9/dist-packages (from -r cs224u/requirements.txt (line 5)) (3.8.1)\n",
            "Requirement already satisfied: pytest>=7.1 in /usr/local/lib/python3.9/dist-packages (from -r cs224u/requirements.txt (line 6)) (7.2.2)\n",
            "Collecting jupyter>=1.0.0\n",
            "  Downloading jupyter-1.0.0-py2.py3-none-any.whl (2.7 kB)\n",
            "Requirement already satisfied: pandas>=1.5 in /usr/local/lib/python3.9/dist-packages (from -r cs224u/requirements.txt (line 8)) (1.5.3)\n",
            "Collecting torch==1.13.1\n",
            "  Downloading torch-1.13.1-cp39-cp39-manylinux1_x86_64.whl (887.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m887.4/887.4 MB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting torchvision==0.14.1\n",
            "  Downloading torchvision-0.14.1-cp39-cp39-manylinux1_x86_64.whl (24.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.2/24.2 MB\u001b[0m \u001b[31m57.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting transformers==4.26.1\n",
            "  Downloading transformers-4.26.1-py3-none-any.whl (6.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.3/6.3 MB\u001b[0m \u001b[31m86.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting datasets==2.10.1\n",
            "  Downloading datasets-2.10.1-py3-none-any.whl (469 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m469.0/469.0 kB\u001b[0m \u001b[31m39.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: spacy==3.5.1 in /usr/local/lib/python3.9/dist-packages (from -r cs224u/requirements.txt (line 13)) (3.5.1)\n",
            "Collecting nvidia-cuda-runtime-cu11==11.7.99\n",
            "  Downloading nvidia_cuda_runtime_cu11-11.7.99-py3-none-manylinux1_x86_64.whl (849 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m849.3/849.3 kB\u001b[0m \u001b[31m50.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: typing-extensions in /usr/local/lib/python3.9/dist-packages (from torch==1.13.1->-r cs224u/requirements.txt (line 9)) (4.5.0)\n",
            "Collecting nvidia-cudnn-cu11==8.5.0.96\n",
            "  Downloading nvidia_cudnn_cu11-8.5.0.96-2-py3-none-manylinux1_x86_64.whl (557.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m557.1/557.1 MB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cublas-cu11==11.10.3.66\n",
            "  Downloading nvidia_cublas_cu11-11.10.3.66-py3-none-manylinux1_x86_64.whl (317.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m317.1/317.1 MB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cuda-nvrtc-cu11==11.7.99\n",
            "  Downloading nvidia_cuda_nvrtc_cu11-11.7.99-2-py3-none-manylinux1_x86_64.whl (21.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.0/21.0 MB\u001b[0m \u001b[31m68.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.9/dist-packages (from torchvision==0.14.1->-r cs224u/requirements.txt (line 10)) (8.4.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from torchvision==0.14.1->-r cs224u/requirements.txt (line 10)) (2.27.1)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1\n",
            "  Downloading tokenizers-0.13.3-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m89.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.9/dist-packages (from transformers==4.26.1->-r cs224u/requirements.txt (line 11)) (4.65.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.9/dist-packages (from transformers==4.26.1->-r cs224u/requirements.txt (line 11)) (2022.10.31)\n",
            "Collecting huggingface-hub<1.0,>=0.11.0\n",
            "  Downloading huggingface_hub-0.13.4-py3-none-any.whl (200 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m200.1/200.1 kB\u001b[0m \u001b[31m24.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.9/dist-packages (from transformers==4.26.1->-r cs224u/requirements.txt (line 11)) (23.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from transformers==4.26.1->-r cs224u/requirements.txt (line 11)) (3.11.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.9/dist-packages (from transformers==4.26.1->-r cs224u/requirements.txt (line 11)) (6.0)\n",
            "Requirement already satisfied: fsspec[http]>=2021.11.1 in /usr/local/lib/python3.9/dist-packages (from datasets==2.10.1->-r cs224u/requirements.txt (line 12)) (2023.4.0)\n",
            "Collecting dill<0.3.7,>=0.3.0\n",
            "  Downloading dill-0.3.6-py3-none-any.whl (110 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m110.5/110.5 kB\u001b[0m \u001b[31m13.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting aiohttp\n",
            "  Downloading aiohttp-3.8.4-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m56.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting xxhash\n",
            "  Downloading xxhash-3.2.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (212 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m212.2/212.2 kB\u001b[0m \u001b[31m18.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pyarrow>=6.0.0 in /usr/local/lib/python3.9/dist-packages (from datasets==2.10.1->-r cs224u/requirements.txt (line 12)) (9.0.0)\n",
            "Collecting responses<0.19\n",
            "  Downloading responses-0.18.0-py3-none-any.whl (38 kB)\n",
            "Collecting multiprocess\n",
            "  Downloading multiprocess-0.70.14-py39-none-any.whl (132 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m132.9/132.9 kB\u001b[0m \u001b[31m15.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.9/dist-packages (from spacy==3.5.1->-r cs224u/requirements.txt (line 13)) (1.1.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.9/dist-packages (from spacy==3.5.1->-r cs224u/requirements.txt (line 13)) (3.1.2)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in /usr/local/lib/python3.9/dist-packages (from spacy==3.5.1->-r cs224u/requirements.txt (line 13)) (1.10.7)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.9/dist-packages (from spacy==3.5.1->-r cs224u/requirements.txt (line 13)) (3.0.12)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.9/dist-packages (from spacy==3.5.1->-r cs224u/requirements.txt (line 13)) (2.0.8)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.9/dist-packages (from spacy==3.5.1->-r cs224u/requirements.txt (line 13)) (2.4.6)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.9/dist-packages (from spacy==3.5.1->-r cs224u/requirements.txt (line 13)) (2.0.7)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.9/dist-packages (from spacy==3.5.1->-r cs224u/requirements.txt (line 13)) (1.0.4)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.9/dist-packages (from spacy==3.5.1->-r cs224u/requirements.txt (line 13)) (6.3.0)\n",
            "Requirement already satisfied: thinc<8.2.0,>=8.1.8 in /usr/local/lib/python3.9/dist-packages (from spacy==3.5.1->-r cs224u/requirements.txt (line 13)) (8.1.9)\n",
            "Requirement already satisfied: pathy>=0.10.0 in /usr/local/lib/python3.9/dist-packages (from spacy==3.5.1->-r cs224u/requirements.txt (line 13)) (0.10.1)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.9/dist-packages (from spacy==3.5.1->-r cs224u/requirements.txt (line 13)) (3.0.8)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.9/dist-packages (from spacy==3.5.1->-r cs224u/requirements.txt (line 13)) (67.6.1)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.9/dist-packages (from spacy==3.5.1->-r cs224u/requirements.txt (line 13)) (1.0.9)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.9/dist-packages (from spacy==3.5.1->-r cs224u/requirements.txt (line 13)) (3.3.0)\n",
            "Requirement already satisfied: typer<0.8.0,>=0.3.0 in /usr/local/lib/python3.9/dist-packages (from spacy==3.5.1->-r cs224u/requirements.txt (line 13)) (0.7.0)\n",
            "Requirement already satisfied: wheel in /usr/local/lib/python3.9/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch==1.13.1->-r cs224u/requirements.txt (line 9)) (0.40.0)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.9/dist-packages (from matplotlib>=3.7.0->-r cs224u/requirements.txt (line 3)) (2.8.2)\n",
            "Requirement already satisfied: importlib-resources>=3.2.0 in /usr/local/lib/python3.9/dist-packages (from matplotlib>=3.7.0->-r cs224u/requirements.txt (line 3)) (5.12.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.9/dist-packages (from matplotlib>=3.7.0->-r cs224u/requirements.txt (line 3)) (3.0.9)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.9/dist-packages (from matplotlib>=3.7.0->-r cs224u/requirements.txt (line 3)) (1.0.7)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.9/dist-packages (from matplotlib>=3.7.0->-r cs224u/requirements.txt (line 3)) (0.11.0)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.9/dist-packages (from matplotlib>=3.7.0->-r cs224u/requirements.txt (line 3)) (4.39.3)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.9/dist-packages (from matplotlib>=3.7.0->-r cs224u/requirements.txt (line 3)) (1.4.4)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.9/dist-packages (from scikit-learn>=1.0.2->-r cs224u/requirements.txt (line 4)) (1.2.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.9/dist-packages (from scikit-learn>=1.0.2->-r cs224u/requirements.txt (line 4)) (3.1.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.9/dist-packages (from nltk>=3.7->-r cs224u/requirements.txt (line 5)) (8.1.3)\n",
            "Requirement already satisfied: exceptiongroup>=1.0.0rc8 in /usr/local/lib/python3.9/dist-packages (from pytest>=7.1->-r cs224u/requirements.txt (line 6)) (1.1.1)\n",
            "Requirement already satisfied: tomli>=1.0.0 in /usr/local/lib/python3.9/dist-packages (from pytest>=7.1->-r cs224u/requirements.txt (line 6)) (2.0.1)\n",
            "Requirement already satisfied: pluggy<2.0,>=0.12 in /usr/local/lib/python3.9/dist-packages (from pytest>=7.1->-r cs224u/requirements.txt (line 6)) (1.0.0)\n",
            "Requirement already satisfied: iniconfig in /usr/local/lib/python3.9/dist-packages (from pytest>=7.1->-r cs224u/requirements.txt (line 6)) (2.0.0)\n",
            "Requirement already satisfied: attrs>=19.2.0 in /usr/local/lib/python3.9/dist-packages (from pytest>=7.1->-r cs224u/requirements.txt (line 6)) (22.2.0)\n",
            "Requirement already satisfied: nbconvert in /usr/local/lib/python3.9/dist-packages (from jupyter>=1.0.0->-r cs224u/requirements.txt (line 7)) (6.5.4)\n",
            "Requirement already satisfied: ipykernel in /usr/local/lib/python3.9/dist-packages (from jupyter>=1.0.0->-r cs224u/requirements.txt (line 7)) (5.5.6)\n",
            "Requirement already satisfied: notebook in /usr/local/lib/python3.9/dist-packages (from jupyter>=1.0.0->-r cs224u/requirements.txt (line 7)) (6.4.8)\n",
            "Requirement already satisfied: jupyter-console in /usr/local/lib/python3.9/dist-packages (from jupyter>=1.0.0->-r cs224u/requirements.txt (line 7)) (6.1.0)\n",
            "Requirement already satisfied: ipywidgets in /usr/local/lib/python3.9/dist-packages (from jupyter>=1.0.0->-r cs224u/requirements.txt (line 7)) (7.7.1)\n",
            "Collecting qtconsole\n",
            "  Downloading qtconsole-5.4.2-py3-none-any.whl (121 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.2/121.2 kB\u001b[0m \u001b[31m14.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.9/dist-packages (from pandas>=1.5->-r cs224u/requirements.txt (line 8)) (2022.7.1)\n",
            "Collecting backoff\n",
            "  Downloading backoff-2.2.1-py3-none-any.whl (15 kB)\n",
            "Collecting openai\n",
            "  Downloading openai-0.27.4-py3-none-any.whl (70 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m70.3/70.3 kB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting ujson\n",
            "  Downloading ujson-5.7.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (52 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m52.8/52.8 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting yarl<2.0,>=1.0\n",
            "  Downloading yarl-1.8.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (264 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m264.6/264.6 kB\u001b[0m \u001b[31m28.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting aiosignal>=1.1.2\n",
            "  Downloading aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.9/dist-packages (from aiohttp->datasets==2.10.1->-r cs224u/requirements.txt (line 12)) (2.0.12)\n",
            "Collecting multidict<7.0,>=4.5\n",
            "  Downloading multidict-6.0.4-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (114 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m114.2/114.2 kB\u001b[0m \u001b[31m13.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting frozenlist>=1.1.1\n",
            "  Downloading frozenlist-1.3.3-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (158 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m158.8/158.8 kB\u001b[0m \u001b[31m18.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting async-timeout<5.0,>=4.0.0a3\n",
            "  Downloading async_timeout-4.0.2-py3-none-any.whl (5.8 kB)\n",
            "Requirement already satisfied: zipp>=3.1.0 in /usr/local/lib/python3.9/dist-packages (from importlib-resources>=3.2.0->matplotlib>=3.7.0->-r cs224u/requirements.txt (line 3)) (3.15.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.9/dist-packages (from python-dateutil>=2.7->matplotlib>=3.7.0->-r cs224u/requirements.txt (line 3)) (1.16.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests->torchvision==0.14.1->-r cs224u/requirements.txt (line 10)) (3.4)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests->torchvision==0.14.1->-r cs224u/requirements.txt (line 10)) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests->torchvision==0.14.1->-r cs224u/requirements.txt (line 10)) (2022.12.7)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.9/dist-packages (from thinc<8.2.0,>=8.1.8->spacy==3.5.1->-r cs224u/requirements.txt (line 13)) (0.0.4)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.9/dist-packages (from thinc<8.2.0,>=8.1.8->spacy==3.5.1->-r cs224u/requirements.txt (line 13)) (0.7.9)\n",
            "Requirement already satisfied: traitlets>=4.1.0 in /usr/local/lib/python3.9/dist-packages (from ipykernel->jupyter>=1.0.0->-r cs224u/requirements.txt (line 7)) (5.7.1)\n",
            "Requirement already satisfied: ipython>=5.0.0 in /usr/local/lib/python3.9/dist-packages (from ipykernel->jupyter>=1.0.0->-r cs224u/requirements.txt (line 7)) (7.34.0)\n",
            "Requirement already satisfied: jupyter-client in /usr/local/lib/python3.9/dist-packages (from ipykernel->jupyter>=1.0.0->-r cs224u/requirements.txt (line 7)) (6.1.12)\n",
            "Requirement already satisfied: tornado>=4.2 in /usr/local/lib/python3.9/dist-packages (from ipykernel->jupyter>=1.0.0->-r cs224u/requirements.txt (line 7)) (6.2)\n",
            "Requirement already satisfied: ipython-genutils in /usr/local/lib/python3.9/dist-packages (from ipykernel->jupyter>=1.0.0->-r cs224u/requirements.txt (line 7)) (0.2.0)\n",
            "Requirement already satisfied: widgetsnbextension~=3.6.0 in /usr/local/lib/python3.9/dist-packages (from ipywidgets->jupyter>=1.0.0->-r cs224u/requirements.txt (line 7)) (3.6.4)\n",
            "Requirement already satisfied: jupyterlab-widgets>=1.0.0 in /usr/local/lib/python3.9/dist-packages (from ipywidgets->jupyter>=1.0.0->-r cs224u/requirements.txt (line 7)) (3.0.7)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.9/dist-packages (from jinja2->spacy==3.5.1->-r cs224u/requirements.txt (line 13)) (2.1.2)\n",
            "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /usr/local/lib/python3.9/dist-packages (from jupyter-console->jupyter>=1.0.0->-r cs224u/requirements.txt (line 7)) (3.0.38)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.9/dist-packages (from jupyter-console->jupyter>=1.0.0->-r cs224u/requirements.txt (line 7)) (2.14.0)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.9/dist-packages (from nbconvert->jupyter>=1.0.0->-r cs224u/requirements.txt (line 7)) (6.0.0)\n",
            "Requirement already satisfied: tinycss2 in /usr/local/lib/python3.9/dist-packages (from nbconvert->jupyter>=1.0.0->-r cs224u/requirements.txt (line 7)) (1.2.1)\n",
            "Requirement already satisfied: defusedxml in /usr/local/lib/python3.9/dist-packages (from nbconvert->jupyter>=1.0.0->-r cs224u/requirements.txt (line 7)) (0.7.1)\n",
            "Requirement already satisfied: mistune<2,>=0.8.1 in /usr/local/lib/python3.9/dist-packages (from nbconvert->jupyter>=1.0.0->-r cs224u/requirements.txt (line 7)) (0.8.4)\n",
            "Requirement already satisfied: nbclient>=0.5.0 in /usr/local/lib/python3.9/dist-packages (from nbconvert->jupyter>=1.0.0->-r cs224u/requirements.txt (line 7)) (0.7.3)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.9/dist-packages (from nbconvert->jupyter>=1.0.0->-r cs224u/requirements.txt (line 7)) (4.11.2)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.9/dist-packages (from nbconvert->jupyter>=1.0.0->-r cs224u/requirements.txt (line 7)) (4.9.2)\n",
            "Requirement already satisfied: pandocfilters>=1.4.1 in /usr/local/lib/python3.9/dist-packages (from nbconvert->jupyter>=1.0.0->-r cs224u/requirements.txt (line 7)) (1.5.0)\n",
            "Requirement already satisfied: jupyter-core>=4.7 in /usr/local/lib/python3.9/dist-packages (from nbconvert->jupyter>=1.0.0->-r cs224u/requirements.txt (line 7)) (5.3.0)\n",
            "Requirement already satisfied: nbformat>=5.1 in /usr/local/lib/python3.9/dist-packages (from nbconvert->jupyter>=1.0.0->-r cs224u/requirements.txt (line 7)) (5.8.0)\n",
            "Requirement already satisfied: entrypoints>=0.2.2 in /usr/local/lib/python3.9/dist-packages (from nbconvert->jupyter>=1.0.0->-r cs224u/requirements.txt (line 7)) (0.4)\n",
            "Requirement already satisfied: jupyterlab-pygments in /usr/local/lib/python3.9/dist-packages (from nbconvert->jupyter>=1.0.0->-r cs224u/requirements.txt (line 7)) (0.2.2)\n",
            "Requirement already satisfied: argon2-cffi in /usr/local/lib/python3.9/dist-packages (from notebook->jupyter>=1.0.0->-r cs224u/requirements.txt (line 7)) (21.3.0)\n",
            "Requirement already satisfied: terminado>=0.8.3 in /usr/local/lib/python3.9/dist-packages (from notebook->jupyter>=1.0.0->-r cs224u/requirements.txt (line 7)) (0.17.1)\n",
            "Requirement already satisfied: Send2Trash>=1.8.0 in /usr/local/lib/python3.9/dist-packages (from notebook->jupyter>=1.0.0->-r cs224u/requirements.txt (line 7)) (1.8.0)\n",
            "Requirement already satisfied: prometheus-client in /usr/local/lib/python3.9/dist-packages (from notebook->jupyter>=1.0.0->-r cs224u/requirements.txt (line 7)) (0.16.0)\n",
            "Requirement already satisfied: pyzmq>=17 in /usr/local/lib/python3.9/dist-packages (from notebook->jupyter>=1.0.0->-r cs224u/requirements.txt (line 7)) (23.2.1)\n",
            "Requirement already satisfied: nest-asyncio>=1.5 in /usr/local/lib/python3.9/dist-packages (from notebook->jupyter>=1.0.0->-r cs224u/requirements.txt (line 7)) (1.5.6)\n",
            "Collecting qtpy>=2.0.1\n",
            "  Downloading QtPy-2.3.1-py3-none-any.whl (84 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.9/84.9 kB\u001b[0m \u001b[31m11.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.9/dist-packages (from ipython>=5.0.0->ipykernel->jupyter>=1.0.0->-r cs224u/requirements.txt (line 7)) (4.8.0)\n",
            "Requirement already satisfied: matplotlib-inline in /usr/local/lib/python3.9/dist-packages (from ipython>=5.0.0->ipykernel->jupyter>=1.0.0->-r cs224u/requirements.txt (line 7)) (0.1.6)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.9/dist-packages (from ipython>=5.0.0->ipykernel->jupyter>=1.0.0->-r cs224u/requirements.txt (line 7)) (0.7.5)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.9/dist-packages (from ipython>=5.0.0->ipykernel->jupyter>=1.0.0->-r cs224u/requirements.txt (line 7)) (4.4.2)\n",
            "Collecting jedi>=0.16\n",
            "  Downloading jedi-0.18.2-py2.py3-none-any.whl (1.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m74.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: backcall in /usr/local/lib/python3.9/dist-packages (from ipython>=5.0.0->ipykernel->jupyter>=1.0.0->-r cs224u/requirements.txt (line 7)) (0.2.0)\n",
            "Requirement already satisfied: platformdirs>=2.5 in /usr/local/lib/python3.9/dist-packages (from jupyter-core>=4.7->nbconvert->jupyter>=1.0.0->-r cs224u/requirements.txt (line 7)) (3.2.0)\n",
            "Requirement already satisfied: jsonschema>=2.6 in /usr/local/lib/python3.9/dist-packages (from nbformat>=5.1->nbconvert->jupyter>=1.0.0->-r cs224u/requirements.txt (line 7)) (4.3.3)\n",
            "Requirement already satisfied: fastjsonschema in /usr/local/lib/python3.9/dist-packages (from nbformat>=5.1->nbconvert->jupyter>=1.0.0->-r cs224u/requirements.txt (line 7)) (2.16.3)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.9/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->jupyter-console->jupyter>=1.0.0->-r cs224u/requirements.txt (line 7)) (0.2.6)\n",
            "Requirement already satisfied: ptyprocess in /usr/local/lib/python3.9/dist-packages (from terminado>=0.8.3->notebook->jupyter>=1.0.0->-r cs224u/requirements.txt (line 7)) (0.7.0)\n",
            "Requirement already satisfied: argon2-cffi-bindings in /usr/local/lib/python3.9/dist-packages (from argon2-cffi->notebook->jupyter>=1.0.0->-r cs224u/requirements.txt (line 7)) (21.2.0)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.9/dist-packages (from beautifulsoup4->nbconvert->jupyter>=1.0.0->-r cs224u/requirements.txt (line 7)) (2.4)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.9/dist-packages (from bleach->nbconvert->jupyter>=1.0.0->-r cs224u/requirements.txt (line 7)) (0.5.1)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.0 in /usr/local/lib/python3.9/dist-packages (from jedi>=0.16->ipython>=5.0.0->ipykernel->jupyter>=1.0.0->-r cs224u/requirements.txt (line 7)) (0.8.3)\n",
            "Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in /usr/local/lib/python3.9/dist-packages (from jsonschema>=2.6->nbformat>=5.1->nbconvert->jupyter>=1.0.0->-r cs224u/requirements.txt (line 7)) (0.19.3)\n",
            "Requirement already satisfied: cffi>=1.0.1 in /usr/local/lib/python3.9/dist-packages (from argon2-cffi-bindings->argon2-cffi->notebook->jupyter>=1.0.0->-r cs224u/requirements.txt (line 7)) (1.15.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.9/dist-packages (from cffi>=1.0.1->argon2-cffi-bindings->argon2-cffi->notebook->jupyter>=1.0.0->-r cs224u/requirements.txt (line 7)) (2.21)\n",
            "Building wheels for collected packages: dsp-ml\n",
            "  Building wheel for dsp-ml (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for dsp-ml: filename=dsp_ml-0.1.4-py3-none-any.whl size=36188 sha256=5f29c78ab4dbde4ae131bb3b279a4947437ecda205cc4b2748fda4675fbcff5f\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-lcfprert/wheels/12/1f/ed/f377a54212af7d154cf5f96b3fce45f04b42916c68a7410407\n",
            "Successfully built dsp-ml\n",
            "Installing collected packages: tokenizers, xxhash, ujson, qtpy, nvidia-cuda-runtime-cu11, nvidia-cuda-nvrtc-cu11, nvidia-cublas-cu11, multidict, jedi, frozenlist, dill, backoff, async-timeout, yarl, responses, nvidia-cudnn-cu11, multiprocess, huggingface-hub, aiosignal, transformers, torch, aiohttp, torchvision, qtconsole, openai, datasets, jupyter, dsp-ml\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 2.0.0+cu118\n",
            "    Uninstalling torch-2.0.0+cu118:\n",
            "      Successfully uninstalled torch-2.0.0+cu118\n"
          ]
        }
      ],
      "source": [
        "try:\n",
        "    # Sort of randomly chosen import to see whether the requirements\n",
        "    # are met:\n",
        "    import datasets\n",
        "except ModuleNotFoundError:\n",
        "    !git clone https://github.com/cgpotts/cs224u/\n",
        "    !pip install -r cs224u/requirements.txt\n",
        "    import sys\n",
        "    sys.path.append(\"cs224u\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pyAzJmyYSNMP"
      },
      "outputs": [],
      "source": [
        "from collections import defaultdict, Counter\n",
        "from datasets import load_dataset\n",
        "import pandas as pd\n",
        "from sklearn.feature_extraction import DictVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "import torch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CdJba1bGJ8hN"
      },
      "source": [
        "## Datasets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vGAwU8KmJ8hN"
      },
      "source": [
        "### DynaSent round 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JWRlPgL08TZW"
      },
      "source": [
        "The DynaSent dataset of [Potts, Wu, et al. 2021](https://aclanthology.org/2021.acl-long.186/) is a ternary sentiment benchmark consisting of two rounds (so far). The dataset is available on [Hugging Face](https://huggingface.co/datasets/dynabench/dynasent)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NsZ-CFMO8TZW"
      },
      "source": [
        "For Round 1, the authors collected sentences from the [Yelp Academic Dataset](https://www.yelp.com/dataset) that fooled a top-performing sentiment model but were intuitive for humans. The model was used only to heuristically find the examples. Crowdworkers multiply-labeled all of them."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W-lZ67jq8TZW"
      },
      "source": [
        "The round contains a lot of metadata that could be useful for developing sentiment models. We will focus on just the sentences and labels, but you are free to make use of this additional metadata in developing uour system."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "referenced_widgets": [
            "d11843b845684ee5ba87dce73d7b9a14",
            "570ca5b0c6e942d0ba3233d1ac3235da",
            "333eb548116446b689c7ee72f7466221",
            "b38fba8028f1407fb4bf2f1409a6a55f",
            "568839c3ca5b40debed7895d88fe3fdf",
            "ac3fca10e6c24bf7864136d968f5f6f6",
            "7a6fbea975024e4e9cc0b610e2f64329",
            "03e15d9401c242fba3872686d5daf473"
          ]
        },
        "id": "j9UcNgx_SZDG",
        "outputId": "4adca388-28a4-4176-b399-8d94102eb509"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d11843b845684ee5ba87dce73d7b9a14",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading builder script:   0%|          | 0.00/16.5k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "570ca5b0c6e942d0ba3233d1ac3235da",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading metadata:   0%|          | 0.00/6.97k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "333eb548116446b689c7ee72f7466221",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading readme:   0%|          | 0.00/13.7k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading and preparing dataset dynasent/dynabench.dynasent.r1.all (download: 16.26 MiB, generated: 23.94 MiB, post-processed: Unknown size, total: 40.20 MiB) to /root/.cache/huggingface/datasets/dynabench___dynasent/dynabench.dynasent.r1.all/1.1.0/ab89971d9ae1aacc59ed44d6855bf0e89167417257e2c2666f38e532148f2967...\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "b38fba8028f1407fb4bf2f1409a6a55f",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading data:   0%|          | 0.00/17.1M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "568839c3ca5b40debed7895d88fe3fdf",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Generating train split:   0%|          | 0/80488 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ac3fca10e6c24bf7864136d968f5f6f6",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Generating validation split:   0%|          | 0/3600 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "7a6fbea975024e4e9cc0b610e2f64329",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Generating test split:   0%|          | 0/3600 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dataset dynasent downloaded and prepared to /root/.cache/huggingface/datasets/dynabench___dynasent/dynabench.dynasent.r1.all/1.1.0/ab89971d9ae1aacc59ed44d6855bf0e89167417257e2c2666f38e532148f2967. Subsequent calls will reuse this data.\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "03e15d9401c242fba3872686d5daf473",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/3 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "dynasent_r1 = load_dataset(\"dynabench/dynasent\", 'dynabench.dynasent.r1.all')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "N7iN05xzSkKo",
        "outputId": "196cfba0-194e-4b4c-a3c9-c07ed3adeb7a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "DatasetDict({\n",
              "    train: Dataset({\n",
              "        features: ['id', 'hit_ids', 'sentence', 'indices_into_review_text', 'model_0_label', 'model_0_probs', 'text_id', 'review_id', 'review_rating', 'label_distribution', 'gold_label', 'metadata'],\n",
              "        num_rows: 80488\n",
              "    })\n",
              "    validation: Dataset({\n",
              "        features: ['id', 'hit_ids', 'sentence', 'indices_into_review_text', 'model_0_label', 'model_0_probs', 'text_id', 'review_id', 'review_rating', 'label_distribution', 'gold_label', 'metadata'],\n",
              "        num_rows: 3600\n",
              "    })\n",
              "    test: Dataset({\n",
              "        features: ['id', 'hit_ids', 'sentence', 'indices_into_review_text', 'model_0_label', 'model_0_probs', 'text_id', 'review_id', 'review_rating', 'label_distribution', 'gold_label', 'metadata'],\n",
              "        num_rows: 3600\n",
              "    })\n",
              "})"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "dynasent_r1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z_iWixX6J8hO"
      },
      "source": [
        "Splits:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "RVjcdb_4SrS2"
      },
      "outputs": [],
      "source": [
        "def print_label_dist(dataset, labelname='gold_label', splitnames=('train', 'validation')):\n",
        "    for splitname in splitnames:\n",
        "        print(splitname)\n",
        "        dist = sorted(Counter(dataset[splitname][labelname]).items())\n",
        "        for k, v in dist:\n",
        "            print(f\"\\t{k:>14s}: {v}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "U3RQdIBRJ8hP",
        "outputId": "e18f6310-2e51-40a3-fded-9e52ca735999"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "train\n",
            "\t      negative: 14021\n",
            "\t       neutral: 45076\n",
            "\t      positive: 21391\n",
            "validation\n",
            "\t      negative: 1200\n",
            "\t       neutral: 1200\n",
            "\t      positive: 1200\n"
          ]
        }
      ],
      "source": [
        "print_label_dist(dynasent_r1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p4WFt0C6J8hP"
      },
      "source": [
        "### DynaSent round 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jrGQTWqB8TZX"
      },
      "source": [
        "DynaSent Round 2 was created using different methods than Round 1. For Round 2, crowdworkers edited sentences from the Yelp Academic Dataset seeking to achieve a particular sentiment goal (e.g., expressing a positive sentiment) while fooling a top-performing model. This work was done on the [Dynabench](https://dynabench.org) platform. The hope is that this directly adversarial goal will lead to examples that are very hard for present-day models but intuitive for humans. All the examples were multiply-labeled by separate annotators."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "referenced_widgets": [
            "bd644f6579c64e2784e16f3c0e79b39b",
            "57e95e1aa82a4b3798f48d2f34f8f636",
            "7e1517b6e76d486e9f9dd8c792392f07",
            "063f058d58a0492b8fb62bc3849851d6"
          ]
        },
        "id": "iz3F_lviJ8hP",
        "outputId": "acace584-47a3-45f1-c455-eb3e7b4fd52b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading and preparing dataset dynasent/dynabench.dynasent.r2.all (download: 16.26 MiB, generated: 4.89 MiB, post-processed: Unknown size, total: 21.15 MiB) to /root/.cache/huggingface/datasets/dynabench___dynasent/dynabench.dynasent.r2.all/1.1.0/ab89971d9ae1aacc59ed44d6855bf0e89167417257e2c2666f38e532148f2967...\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "bd644f6579c64e2784e16f3c0e79b39b",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Generating train split:   0%|          | 0/13065 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "57e95e1aa82a4b3798f48d2f34f8f636",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Generating validation split:   0%|          | 0/720 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "7e1517b6e76d486e9f9dd8c792392f07",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Generating test split:   0%|          | 0/720 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dataset dynasent downloaded and prepared to /root/.cache/huggingface/datasets/dynabench___dynasent/dynabench.dynasent.r2.all/1.1.0/ab89971d9ae1aacc59ed44d6855bf0e89167417257e2c2666f38e532148f2967. Subsequent calls will reuse this data.\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "063f058d58a0492b8fb62bc3849851d6",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/3 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "dynasent_r2 = load_dataset(\"dynabench/dynasent\", 'dynabench.dynasent.r2.all')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "Up68q68dJ8hP",
        "outputId": "46d74fc3-ea5a-4eb8-f3f9-0280f40aaff3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "train\n",
            "\t      negative: 4579\n",
            "\t       neutral: 2448\n",
            "\t      positive: 6038\n",
            "validation\n",
            "\t      negative: 240\n",
            "\t       neutral: 240\n",
            "\t      positive: 240\n"
          ]
        }
      ],
      "source": [
        "print_label_dist(dynasent_r2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qeONNIJQJ8hP"
      },
      "source": [
        "### Stanford Sentiment Treebank"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sYlht2_C8TZX"
      },
      "source": [
        "The [Stanford Sentiment Treebank (SST)](http://nlp.stanford.edu/sentiment/) of [Socher et al. 2013](https://aclanthology.org/D13-1170/) is a widely-used resource for evaluating supervised models. It consists of sentences from Rotten Tomatoes Movie Reviews (see [Pang and Lee's project page](https://www.cs.cornell.edu/home/llee/papers/pang-lee-stars.home.html)). We will use the ternary version of the task (SST-3)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nt4KuACV8TZX"
      },
      "source": [
        "SST examples are special in that they are labeled at the phrase-level as well as the sentence level, which provides very extensive and detailed supervision for sentiment. We will use only the sentence-level labels for the homework, but you are free to use the phrase-level labels as well in designing your original system. (To do this, you will need to get the dataset from the above project page, since the Hugging Face SST-3 we are using does not include these labels.)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "referenced_widgets": [
            "ca10062671b84964b1d2237e9f7232bb",
            "cd75940adc414d28adc257fa4b0f255b",
            "3e4d440c380f4ce2b68c009f8d20bbda",
            "1aed1cd4e2294338b6517817dcddb946",
            "c63f273f24604db8bfea65d7eb61bb1c",
            "6d9d2159eb544b8f90ca3d05d368b41c",
            "f082552644ed445ebed38e396e236992",
            "fbceabb6f9824769a7b519f0f3e5862e",
            "e742535412be4510a6ce4e66593e857e",
            "4fb0bfc4a31d448995a0926a3e6a854d"
          ]
        },
        "id": "rJROF2MaJ8hP",
        "outputId": "1067046e-8e95-4a30-87ac-6498e533a14d"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ca10062671b84964b1d2237e9f7232bb",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading readme:   0%|          | 0.00/421 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading and preparing dataset json/SetFit--sst5 to /root/.cache/huggingface/datasets/SetFit___json/SetFit--sst5-4c07b9d5881ae209/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51...\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "cd75940adc414d28adc257fa4b0f255b",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading data files:   0%|          | 0/3 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "3e4d440c380f4ce2b68c009f8d20bbda",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading data:   0%|          | 0.00/1.32M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "1aed1cd4e2294338b6517817dcddb946",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading data:   0%|          | 0.00/343k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c63f273f24604db8bfea65d7eb61bb1c",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading data:   0%|          | 0.00/171k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "6d9d2159eb544b8f90ca3d05d368b41c",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Extracting data files:   0%|          | 0/3 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f082552644ed445ebed38e396e236992",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Generating train split: 0 examples [00:00, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "fbceabb6f9824769a7b519f0f3e5862e",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Generating test split: 0 examples [00:00, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e742535412be4510a6ce4e66593e857e",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Generating validation split: 0 examples [00:00, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dataset json downloaded and prepared to /root/.cache/huggingface/datasets/SetFit___json/SetFit--sst5-4c07b9d5881ae209/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51. Subsequent calls will reuse this data.\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "4fb0bfc4a31d448995a0926a3e6a854d",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/3 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "sst = load_dataset(\"SetFit/sst5\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "vw0I60nOJ8hQ",
        "outputId": "43d95cad-60d9-4259-d0e5-280a6223dcdf"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "DatasetDict({\n",
              "    train: Dataset({\n",
              "        features: ['text', 'label', 'label_text'],\n",
              "        num_rows: 8544\n",
              "    })\n",
              "    test: Dataset({\n",
              "        features: ['text', 'label', 'label_text'],\n",
              "        num_rows: 2210\n",
              "    })\n",
              "    validation: Dataset({\n",
              "        features: ['text', 'label', 'label_text'],\n",
              "        num_rows: 1101\n",
              "    })\n",
              "})"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "sst"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pqGEinGf8TZY"
      },
      "source": [
        "Out of the box, this is a five-way task:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "hxmbH6qkJ8hQ",
        "outputId": "48915da8-64c7-49b8-bb3f-cd41082cb174"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "train\n",
            "\t      negative: 2218\n",
            "\t       neutral: 1624\n",
            "\t      positive: 2322\n",
            "\t very negative: 1092\n",
            "\t very positive: 1288\n",
            "validation\n",
            "\t      negative: 289\n",
            "\t       neutral: 229\n",
            "\t      positive: 279\n",
            "\t very negative: 139\n",
            "\t very positive: 165\n"
          ]
        }
      ],
      "source": [
        "print_label_dist(sst, labelname='label_text')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nv3FJVyz8TZY"
      },
      "source": [
        "The above labels are not aligned with our ternary task, and the dataset distribution uses slightly different keys from those of DynaSent. The following code converts the dataset to SST-3 and also aligns the dataset keys:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "WDgWI8IzJ8hQ"
      },
      "outputs": [],
      "source": [
        "def convert_sst_label(s):\n",
        "    return s.split(\" \")[-1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "UDT2FS0RJ8hQ"
      },
      "outputs": [],
      "source": [
        "for splitname in ('train', 'validation', 'test'):\n",
        "    dist = [convert_sst_label(s) for s in sst[splitname]['label_text']]\n",
        "    sst[splitname] = sst[splitname].add_column('gold_label', dist)\n",
        "    sst[splitname] = sst[splitname].add_column('sentence', sst[splitname]['text'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "w-dl9asPJ8hQ",
        "outputId": "7d20574d-9183-4f9b-a5c9-c6a505715ddf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "train\n",
            "\t      negative: 3310\n",
            "\t       neutral: 1624\n",
            "\t      positive: 3610\n",
            "validation\n",
            "\t      negative: 428\n",
            "\t       neutral: 229\n",
            "\t      positive: 444\n"
          ]
        }
      ],
      "source": [
        "print_label_dist(sst)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BjSgq8wE8TZY"
      },
      "source": [
        "## Question 1: Linear classifiers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vHPgdpfuJ8hQ"
      },
      "source": [
        "Our first set of experiments will use simple linear classifiers with sparse representations derived from counting unigrams. These experiments will introduce some useful techniques and provide a baseline for original systems. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NYq-zsNz8TZY"
      },
      "source": [
        "### Background: Feature functions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kY2X8P2b8TZY"
      },
      "source": [
        "The following is a flexible format for writing feature functions in the context of scikit-learn modeling. The function maps a string to a count dictionary, using the simple procedure of splitting on whitespace and counting the resulting elements:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uMognAf88TZY"
      },
      "outputs": [],
      "source": [
        "def unigrams_phi(s):\n",
        "    \"\"\"The basis for a unigrams feature function.\n",
        "\n",
        "    Downcases all tokens.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    s : str\n",
        "        The example to represent\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    Counter\n",
        "        A map from tokens (str) to their counts in `text`\n",
        "\n",
        "    \"\"\"\n",
        "    return Counter(s.lower().split())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R2mpmJyN8TZY"
      },
      "source": [
        "Quick example:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XyKI2y_E8TZY",
        "outputId": "70c5b5b9-15cf-47f9-ba39-2a6b44f97696"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Counter({\"here's\": 1,\n",
              "         'an': 2,\n",
              "         'example': 1,\n",
              "         'with': 1,\n",
              "         'emoticon': 1,\n",
              "         ':)!': 1})"
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "unigrams_phi(\"Here's an example with an emoticon :)!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gti3zBx28TZY"
      },
      "source": [
        "### Background: Feature space vectorization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ezpuPuRi8TZY"
      },
      "source": [
        "Functions like `unigrams_phi`  are just the __basis__ for feature representations. In truth, our models typically don't represent examples as dictionaries, but rather as vectors embedded in a matrix. In general, to manage the translation from dictionaries to vectors, we use [sklearn.feature_extraction.DictVectorizer](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.DictVectorizer.html) instances. Here's a brief overview of how these work:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NdJMOiFQ8TZZ"
      },
      "source": [
        "To start, suppose that we had just two examples to represent, and our feature function mapped them to the following list of dictionaries:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UBtU5Fm48TZZ"
      },
      "outputs": [],
      "source": [
        "train_feats = [\n",
        "    {'a': 1, 'b': 1},\n",
        "    {'b': 1, 'c': 2}]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F1_jMEQy8TZZ"
      },
      "source": [
        "Now we create a `DictVectorizer`. So that we can more easily inspect the resulting matrix, I've set `sparse=False`, so that the return value is a dense matrix. For real problems, you'll probably want to use `sparse=True`, as it will be vastly more efficient for the very sparse feature matrices that you are likely to be creating."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ki9vSIOr8TZZ"
      },
      "outputs": [],
      "source": [
        "vec = DictVectorizer(sparse=False)  # Use `sparse=True` for real problems!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tmwuuMaj8TZZ"
      },
      "source": [
        "The `fit_transform` method maps our list of dictionaries to a matrix:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1XnPvTYc8TZZ"
      },
      "outputs": [],
      "source": [
        "X_train = vec.fit_transform(train_feats)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7sZIgORU8TZZ"
      },
      "source": [
        "Here I'll create a `pd.Datafame` just to help us inspect `X_train`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 112
        },
        "id": "L_rKePun8TZZ",
        "outputId": "dd98406f-7b62-48f0-f4ba-c21819de1361"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-ac3d2d2f-601a-417b-8d52-0266c6b8782e\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>a</th>\n",
              "      <th>b</th>\n",
              "      <th>c</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>2.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-ac3d2d2f-601a-417b-8d52-0266c6b8782e')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-ac3d2d2f-601a-417b-8d52-0266c6b8782e button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-ac3d2d2f-601a-417b-8d52-0266c6b8782e');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "     a    b    c\n",
              "0  1.0  1.0  0.0\n",
              "1  0.0  1.0  2.0"
            ]
          },
          "execution_count": 24,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "pd.DataFrame(X_train, columns=vec.get_feature_names_out())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U5-bo5j48TZZ"
      },
      "source": [
        "Now we can see that, intuitively, the feature called \"a\" is embedded in the first column, \"b\" in the second column, and \"c\" in the third."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pDD6nfId8TZZ"
      },
      "source": [
        "Now suppose we have some new test examples:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KB-KlOL48TZZ"
      },
      "outputs": [],
      "source": [
        "test_feats = [\n",
        "    {'a': 2, 'c': 1},\n",
        "    {'a': 4, 'b': 2, 'd': 1}]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wDXrug6V8TZZ"
      },
      "source": [
        "If we have trained a model on `X_train`, then it will not have any way to deal with this new feature \"d\". This shows that we need to embed `test_feats` in the same space as `X_train`. To do this, one just calls `transform` on the existing vectorizer:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7a798ljV8TZZ"
      },
      "outputs": [],
      "source": [
        "X_test = vec.transform(test_feats)  # Not `fit_transform`!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 112
        },
        "id": "xOoZ0pUN8TZZ",
        "outputId": "b27b7c4d-eb50-4343-ba95-cb27e92abd1c"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-a182fbf4-62f2-4510-be1a-8d097d31e03f\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>a</th>\n",
              "      <th>b</th>\n",
              "      <th>c</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>4.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-a182fbf4-62f2-4510-be1a-8d097d31e03f')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-a182fbf4-62f2-4510-be1a-8d097d31e03f button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-a182fbf4-62f2-4510-be1a-8d097d31e03f');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "     a    b    c\n",
              "0  2.0  0.0  1.0\n",
              "1  4.0  2.0  0.0"
            ]
          },
          "execution_count": 27,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "pd.DataFrame(X_test, columns=vec.get_feature_names_out())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F5DA5yZd8TZZ"
      },
      "source": [
        "The most common mistake with `DictVectorizer` is calling `fit_transform` on test examples. This will wipe out the existing representation scheme, replacing it with one that matches the test examples. That will happen silently, but then you'll find that the new representations are incompatible with the model you fit. This is likely to manifest itself as a `ValueError` relating to feature counts. Here's an example that might help you spot this if and when it arises in your own work:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9l-ihCuG8TZZ",
        "outputId": "9bf8e134-6384-4ec1-9dee-3d0e9c26bf68"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ValueError: X has 4 features, but LogisticRegression is expecting 3 features as input.\n"
          ]
        }
      ],
      "source": [
        "toy_mod = LogisticRegression()\n",
        "\n",
        "vec = DictVectorizer(sparse=False)\n",
        "\n",
        "X_train = vec.fit_transform(train_feats)\n",
        "\n",
        "toy_mod.fit(X_train, [0, 1])\n",
        "\n",
        "# Here's the error! Don't use `fit_transform` again! \n",
        "# Use `transform`!\n",
        "X_test = vec.fit_transform(test_feats)\n",
        "\n",
        "try:\n",
        "    toy_mod.predict(X_test)\n",
        "except ValueError as err:\n",
        "    print(\"ValueError: {}\".format(err))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "65SF0YZT8TZZ"
      },
      "source": [
        "### Background: scikit-learn models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xYFpKpHY8TZZ"
      },
      "source": [
        "scikit-learn is an amazing package with, among many other things, an incredible array of classifier model implementations. We're going to use a simple softmax classifier for this homework question, but you will find that you can swap in essentially any scikit-learn classifier and see how it does."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Udn3t3Jr8TZZ"
      },
      "source": [
        "The core rhythm for scikit-learn models:\n",
        "\n",
        "1. Instantiate the model with any hyperparamters.\n",
        "2. `fit` \n",
        "3. `predict`\n",
        "\n",
        "Here's a quick example that also shows off scikit-learn's functionality for creating synthetic datasets and random train/test splits:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_ARHHDfZ8TZa"
      },
      "outputs": [],
      "source": [
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_toy, y_toy = make_classification(\n",
        "    n_samples=200, n_classes=3, \n",
        "    n_informative=15, n_features=20, \n",
        "    weights=[0.2, 0.2, 0.6],\n",
        "    random_state=1)\n",
        "\n",
        "X_toy_train, X_toy_test, y_toy_train, y_toy_test = train_test_split(\n",
        "    X_toy, y_toy, test_size=0.20, stratify=y_toy, random_state=1)\n",
        "\n",
        "toymod = LogisticRegression(penalty='l2', C=1, fit_intercept=True)\n",
        "\n",
        "toymod.fit(X_toy_train, y_toy_train)\n",
        "\n",
        "toypreds = toymod.predict(X_toy_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gFmoPGOH8TZa"
      },
      "source": [
        "### Background: Classifier assessment"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7BUdjzLg8TZa"
      },
      "source": [
        "When assessing a classifier, the best first step is usually to get a classification report:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x3gqrES-8TZa",
        "outputId": "2a65a1a8-408a-48fd-b47f-6eaad2697850"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0      0.444     0.500     0.471         8\n",
            "           1      0.444     0.500     0.471         8\n",
            "           2      0.909     0.833     0.870        24\n",
            "\n",
            "    accuracy                          0.700        40\n",
            "   macro avg      0.599     0.611     0.604        40\n",
            "weighted avg      0.723     0.700     0.710        40\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics import classification_report\n",
        "\n",
        "print(classification_report(y_toy_test, toypreds, digits=3))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hIRfQHFm8TZa"
      },
      "source": [
        "In this course, we will generally focus in the __macro-average F1 score__ (macro avg above). This is simply the mean of the per-class F1 scores, without any attention paid to the overall size of the class. This is our default because, in NLP, we tend to care about small classes as much as (often more than) large classes."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dFSUpYxb8TZa"
      },
      "source": [
        "The scikit-learn implementation of `macro_f1` can be finicky, so our course code provides a convenient wrapper:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-22XwoBV8TZa",
        "outputId": "3502271d-54e8-42d3-e969-995285785678"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0.6035805626598466"
            ]
          },
          "execution_count": 31,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import utils\n",
        "\n",
        "utils.safe_macro_f1(y_toy_test, toypreds)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sz_bohaC8TZa"
      },
      "source": [
        "Note: scikit-learn models have a `score` method. For classifiers, this is set to use `accuracy` by default:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QH2v_hdb8TZa",
        "outputId": "c744a504-76fb-4ba6-cbbb-39cd363c1650"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0.7"
            ]
          },
          "execution_count": 32,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "toymod.score(X_toy_test, y_toy_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_VBpFfWP8TZa"
      },
      "source": [
        "Accuracy generally isn't well-aligned with our goals, so we discourage use of this method (and of accuracy scores in general)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iws5T_Tr8TZa"
      },
      "source": [
        "scikit-learn also makes it very easy to perform automatic hyperparameter tuning. A quick example:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dETu7gsO8TZa"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "params = {'C': (0.1, 0.2, 0.3), 'fit_intercept': [True, False]}\n",
        "\n",
        "toymod_tuned = LogisticRegression()\n",
        "\n",
        "clf = GridSearchCV(toymod_tuned, params, scoring='f1_macro')\n",
        "\n",
        "_ = clf.fit(X_toy, y_toy)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S4b8MNa18TZa"
      },
      "source": [
        "Here's the best model found by this search:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 74
        },
        "id": "GmcFgPkB8TZa",
        "outputId": "3a600ab4-bc4c-4715-bba9-811dd30bbb01"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LogisticRegression(C=0.3, fit_intercept=False)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression(C=0.3, fit_intercept=False)</pre></div></div></div></div></div>"
            ],
            "text/plain": [
              "LogisticRegression(C=0.3, fit_intercept=False)"
            ]
          },
          "execution_count": 34,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "clf.best_estimator_"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B4m8J2gl8TZa"
      },
      "source": [
        "Because we set `scoring='f1_macro'`, the above model was selected using our favored classifier scoring metric:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "egqQPs0j8TZa",
        "outputId": "767a47df-3273-449d-8dd1-551e14fa9dd2"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0.6943888670150135"
            ]
          },
          "execution_count": 35,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "clf.best_score_"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XwF-fn-J8TZb"
      },
      "source": [
        "With this best model in hand, we can perform our usual assessment:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_t0Wm1Z88TZb"
      },
      "outputs": [],
      "source": [
        "bestpreds = clf.best_estimator_.predict(X_toy_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6GBkK6iW8TZb",
        "outputId": "efb09df0-13ad-471d-f41c-9dd5510772de"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0      0.750     0.600     0.667        10\n",
            "           1      0.750     0.750     0.750         8\n",
            "           2      0.833     0.909     0.870        22\n",
            "\n",
            "    accuracy                          0.800        40\n",
            "   macro avg      0.778     0.753     0.762        40\n",
            "weighted avg      0.796     0.800     0.795        40\n",
            "\n"
          ]
        }
      ],
      "source": [
        "print(classification_report(bestpreds, y_toy_test, digits=3))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DryCOvvX8TZb"
      },
      "source": [
        "### Task 1: Feature functions [1 point]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vOY6Z_nM8TZb"
      },
      "source": [
        "The tokenization scheme used by `unigrams_phi` is very basic and leads to unintuitive tokens with punctuation attached to them. Your task here is to complete `tweetgrams_phi`, which should lead to more intuitive results. The task is really just to use the NLTK [TweetTokenizer](https://www.nltk.org/api/nltk.tokenize.casual.html#nltk.tokenize.casual.TweetTokenizer) in place of the simple whitespace tokenization of `unigrams_phi` above."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x3yH6CD48TZb"
      },
      "outputs": [],
      "source": [
        "# Your `tweetgrams_phi` should tokenize data according to this tokenizer from NLTK:\n",
        "from nltk.tokenize import TweetTokenizer\n",
        "\n",
        "def tweetgrams_phi(s, **kwargs):\n",
        "    \"\"\"The basis for a feature function using `TweetTokenizer`.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    s : str\n",
        "    kwargs : dict\n",
        "        Passed to `TweetTokenizer`\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    Counter\n",
        "        A map from tokens to their counts in `text`\n",
        "\n",
        "    \"\"\"\n",
        "    tknzr = TweetTokenizer(**kwargs)\n",
        "    return Counter(tknzr.tokenize(s))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7D3HIFK38TZb"
      },
      "source": [
        "Here's a test you can use to check that your implementation is correct:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SSpuUofQ8TZb"
      },
      "outputs": [],
      "source": [
        "def test_tweetgrams_phi(func):\n",
        "    examples = [\n",
        "        (\n",
        "            \"Here's an example with an emoticon :)\", \n",
        "            Counter({'an': 2, \"Here's\": 1, 'example': 1, 'with': 1, 'emoticon': 1, ':)': 1})\n",
        "        ),\n",
        "        (\n",
        "            \"The URL is https://pytorch.org!\", \n",
        "            Counter({'The': 1, 'URL': 1, 'is': 1, 'https://pytorch.org': 1, '!': 1})\n",
        "        )\n",
        "    ]\n",
        "    errcount = 0\n",
        "    for ex, expected in examples:\n",
        "        result = func(ex, preserve_case=True)\n",
        "        if result != expected:\n",
        "            errcount += 1\n",
        "            print(f\"Error for `{func.__name__}`: For input {ex}, \"\n",
        "                  f\"expected {expected} but got {result}\")\n",
        "    caps_ex = \"CAPS\"\n",
        "    caps_result = func(caps_ex, preserve_case=False)\n",
        "    caps_expected = Counter({\"caps\": 1})\n",
        "    if caps_result != caps_expected:\n",
        "        errcount += 1\n",
        "        print(f\"Error for `{func.__name__}`: For input {caps_ex}, \"\n",
        "              f\"expected {caps_expected} but got {caps_result}\")    \n",
        "    if errcount == 0:\n",
        "        print(f\"All tests passed for `{func.__name__}`\")    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mypKtoK68TZb",
        "outputId": "8f0cb3a0-a4bd-4d8b-dca7-4b8c0edc460b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "All tests passed for `tweetgrams_phi`\n"
          ]
        }
      ],
      "source": [
        "test_tweetgrams_phi(tweetgrams_phi)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1uIlGqZGJ8hR"
      },
      "source": [
        "### Task 2: Model training [1 point]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CzaPhBIS8TZb"
      },
      "source": [
        "Your task is to complete `train_linear_model`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rtEaAkHtStQg"
      },
      "outputs": [],
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.feature_extraction import DictVectorizer\n",
        "from sklearn.metrics import classification_report"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yfhADdcyJ8hR"
      },
      "outputs": [],
      "source": [
        "def train_linear_model(model, featfunc, train_dataset):\n",
        "    \"\"\"Train an sklearn classifier.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    model : sklearn classifier model\n",
        "    featfunc : func\n",
        "        Maps strings to Counter instances\n",
        "    train_dataset: dict\n",
        "        Must have a key \"sentence\" containing strings that `featfunc` \n",
        "        will process, and a key \"gold_label\" giving labels\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    tuple\n",
        "        * A trained version of `model`\n",
        "        * A fitted `vectorizer` for the train set\n",
        "\n",
        "    \"\"\"\n",
        "    \n",
        "    # Step 1: Featurize all the examples in `train_dataset['sentence']`\n",
        "    train_feats = [featfunc(s) for s in train_dataset['sentence']]\n",
        "\n",
        "    # Step 2: Instantiate and use a `DictVectorizer`:\n",
        "    vec = DictVectorizer(sparse=True) \n",
        "    X_train = vec.fit_transform(train_feats)\n",
        "\n",
        "    # Step 3: Train the model on the feature matrix and\n",
        "    # train_dataset['gold_label']:\n",
        "    model.fit(X_train, train_dataset['gold_label'])\n",
        "\n",
        "    # Step 4: Return (model, vectorizer):\n",
        "    return (model, vec)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rbIjW3aH8TZb"
      },
      "source": [
        "You can use the following test to help ensure that your implementation is correct:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y6NGs-HN8TZb"
      },
      "outputs": [],
      "source": [
        "def test_train_linear_model(func):\n",
        "    train_dataset = {\n",
        "        'sentence': ['A A', 'A B', 'B B', 'B A', 'B'],\n",
        "        'gold_label': [0, 1, 0, 1, 1]}\n",
        "    def featfunc(s):\n",
        "        return Counter(s.split())\n",
        "    model = LogisticRegression()\n",
        "    result = func(model, featfunc, train_dataset)\n",
        "    if not isinstance(result, tuple) or len(result) != 2:\n",
        "        print(f\"Error for `{func.__name__}`: Incorrect return type\")\n",
        "        return\n",
        "    model, vectorizer = result\n",
        "    if not hasattr(vectorizer, 'vocabulary_'):\n",
        "        print(f\"Error for `{func.__name__}`: \"\n",
        "              f\"Second return value is not a trained vectorizer\")\n",
        "        return\n",
        "    if not hasattr(model, 'classes_'):\n",
        "        print(f\"Error for `{func.__name__}`: \"\n",
        "              f\"First return value is not a trained classifier\")\n",
        "        return\n",
        "    print(f\"No errors found for `{func.__name__}`\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aFNCGoTX8TZb",
        "outputId": "6f0731ac-5098-4559-f98b-ddbb4170f85c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "No errors found for `train_linear_model`\n"
          ]
        }
      ],
      "source": [
        "_ = test_train_linear_model(train_linear_model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r2FqGQ5D8TZb"
      },
      "source": [
        "You can now very easily train models on our datasets. Quick example (this shouldn't take more than a couple of minutes to run even on a CPU):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OQoCRRPHJ8hR"
      },
      "outputs": [],
      "source": [
        "lr_unigrams, vec_unigrams = train_linear_model(\n",
        "    LogisticRegression(max_iter=1000), \n",
        "    unigrams_phi, dynasent_r1['train'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "irhvUGtk8TZc"
      },
      "source": [
        "### Task 3: Model assessment [1 point]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eUSVSYMF8TZc"
      },
      "source": [
        "Having now trained a model, we'd like to perform assessments on new data. Your task is to complete the wrapper function `assess_linear_model` to do this. The primary things you need to put into practice are (1) how to use a trained vectorizer on new data and (2) how to make predictions with your trained model. (Both of these steps are reviewed earlier in this notebook.)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GKa7uOwYJ8hR"
      },
      "outputs": [],
      "source": [
        "def assess_linear_model(model, featfunc, vectorizer, assess_dataset):\n",
        "    \"\"\"Assess a trained sklearn model.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    model: trained sklearn model\n",
        "    featfunc : func\n",
        "        Maps strings to count dicts\n",
        "    vectorizer : fitted DictVectorizer\n",
        "    assess_dataset: dict\n",
        "        Must have a key \"sentence\" containing strings that `featfunc` \n",
        "        will process, and a key \"gold_label\" giving labels\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    A classification report (multiline string)\n",
        "\n",
        "    \"\"\"\n",
        "    # Step 1: Featurize the assessment data:\n",
        "    test_feats = [featfunc(s) for s in assess_dataset['sentence']]\n",
        "\n",
        "    # Step 2: Vectorize the assessment data features:\n",
        "    X_test = vectorizer.transform(test_feats)\n",
        "\n",
        "    # Step 3: Make predictions:\n",
        "    preds = model.predict(X_test)\n",
        "\n",
        "    # Step 4: Return a classification report (str):\n",
        "    return classification_report(assess_dataset['gold_label'], preds)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v7nOvOe38TZc"
      },
      "source": [
        "Here's a quick test you can use:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZmsHm5Co8TZc"
      },
      "outputs": [],
      "source": [
        "def test_assess_linear_model(assessfunc, trainfunc):\n",
        "    train_dataset = {\n",
        "        'sentence': ['A A', 'A B', 'B B', 'B A', 'A', 'B'],\n",
        "        'gold_label': [0, 1, 0, 1, 0, 1]}\n",
        "    assess_dataset = {\n",
        "        'sentence': ['A C', 'B A'],\n",
        "        'gold_label': [0, 1]}\n",
        "    def featfunc(s):\n",
        "        return Counter(s.split())\n",
        "    model = LogisticRegression()\n",
        "    model, vectorizer = trainfunc(model, featfunc, train_dataset)\n",
        "    result = assessfunc(model, featfunc, vectorizer, assess_dataset)\n",
        "    errcount = 0\n",
        "    #print(result)\n",
        "    if len(vectorizer.vocabulary_) != 2:\n",
        "        print(f\"Error for `{assessfunc.__name__}`: Unexpected feature count\")\n",
        "        errcount += 1\n",
        "    if 'weighted avg' not in result:\n",
        "        print(f\"Error for `{assessfunc.__name__}`: Unexpected return value\")\n",
        "        errcount += 1\n",
        "    if errcount == 0:\n",
        "        print(f\"No errors found for `{assessfunc.__name__}`\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qd-MLGNQ8TZc",
        "outputId": "bec96d38-d688-474e-dbb6-0636e9906f4a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "No errors found for `assess_linear_model`\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.9/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        }
      ],
      "source": [
        "test_assess_linear_model(assess_linear_model, train_linear_model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F0gdOeCD8TZc"
      },
      "source": [
        "If you trained a model `lr_unigrams` above, you can now easily assess it. An example:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "buRd2jpYJ8hR",
        "outputId": "8bf5f23e-3d8e-410b-8b63-039e20a7eaa9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "    negative       0.76      0.36      0.49      1200\n",
            "     neutral       0.52      0.89      0.66      1200\n",
            "    positive       0.70      0.57      0.63      1200\n",
            "\n",
            "    accuracy                           0.61      3600\n",
            "   macro avg       0.66      0.61      0.59      3600\n",
            "weighted avg       0.66      0.61      0.59      3600\n",
            "\n"
          ]
        }
      ],
      "source": [
        "report = assess_linear_model(\n",
        "    lr_unigrams,\n",
        "    unigrams_phi,\n",
        "    vec_unigrams,\n",
        "    dynasent_r1['validation'])\n",
        "\n",
        "print(report)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ezZAgHCNJ8hS"
      },
      "source": [
        "## Question 2: Transformer fine-tuning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yeeEsTjd8TZc"
      },
      "source": [
        "We're now going to move into a more modern mode: fine-tuning pretrained components."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W4tC3RHf8TZc"
      },
      "source": [
        "We'll use BERT-mini (originally from [the BERT repo](https://github.com/google-research/bert)) for the homework so that we can rapdily develop prototypes. You can then consider scaling up to larger models."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HRf3Is2D8TZc"
      },
      "outputs": [],
      "source": [
        "import transformers\n",
        "from transformers import AutoModel, AutoTokenizer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CzlRDnr08TZc"
      },
      "source": [
        "The `transformers` library does a lot of logging. To avoid ending up with a cluttered notebook, I am changing the logging level. You might want to skip this as you scale up to building production systems, since the logging is very good – it gives you a lot of insights into what the models and code are doing."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KUF_tqWg8TZc"
      },
      "outputs": [],
      "source": [
        "transformers.logging.set_verbosity_error()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a8yB176b8TZc"
      },
      "source": [
        "Here we set ourselves up to use BERT-mini:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187,
          "referenced_widgets": [
            "05bd8598144846d183e71a82a67a6bc0",
            "11beceea418f426d86f9f28ef6f4f8ca",
            "09481ac101eb40e7b3db001e227ed04f",
            "5d01709b419244c68d6ce1e6ff941cf2",
            "9a9c4a540ae344689232f6ae3617d2db",
            "f3f787ef507340559ae03a34422b3f61",
            "6cefcd18445444c2a117e185410f4219",
            "ebc1ce58e78948bead4a2a2a4b2bb5cc",
            "cc3619b8705345688badafe8306c887f",
            "934787727a5a43f2b0d8c2a05f61dbd6",
            "321442faba684d589631eb8cf062ae2b",
            "c193a071ae9649b7bd37cebcdc177531",
            "a987ea5cb2af4023a2ee907a188f4028",
            "18e56f5dcb5047959fb91433722cf982",
            "734131b2a4a64449bacf7720f96f5707",
            "29e3f35345b24155b7a9b3b275e97268",
            "ec80f21750f64f51868fea4c66c88a69",
            "93a8b8e390454f079a917a7b4c2a19f4",
            "f71c5bc693f1439c9e9097b461983298",
            "5f3eea9c9f50414a90012cf4a0bd941a",
            "a446bccf10fd4d3fb0fd479acc31edb2",
            "609ea1432a9f4a3798b6bdc4a876e7cb",
            "06626014e1884b2cada8fc05c65babba",
            "d70b0e902b374ebe976b8eb6e2455b9f",
            "1cea5f29c7c849b8b748596ef92f3b7f",
            "fb05390f8941496f9778f71064f5a23f",
            "98a1f6e511b6431a912ed1559184de3d",
            "10f02f9aeed54f66b265d1fd40bebfd1",
            "c7e64bab54c14e4498a3bb118a9853e9",
            "d53950f033e9462c9bed528d026485ef",
            "18045b61283c4b21bb9ec779b499a8ad",
            "ec6f2b4cdea84002b110c481c7bcf926",
            "050c57c2ad7842f2815cc6e697a1b86b"
          ]
        },
        "id": "Y9KwERym8TZc",
        "outputId": "89817e45-db1e-47a3-de65-fa5edcab700a"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "05bd8598144846d183e71a82a67a6bc0",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading (…)lve/main/config.json:   0%|          | 0.00/286 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c193a071ae9649b7bd37cebcdc177531",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading pytorch_model.bin:   0%|          | 0.00/45.1M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at prajjwal1/bert-mini were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.decoder.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "06626014e1884b2cada8fc05c65babba",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading (…)solve/main/vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "weights_name = \"prajjwal1/bert-mini\"\n",
        "\n",
        "bert = AutoModel.from_pretrained(weights_name)\n",
        "\n",
        "bert_tokenizer = AutoTokenizer.from_pretrained(weights_name)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nxNpIfRH8TZc"
      },
      "source": [
        "### Background: Tokenization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uVj61P4z8TZc"
      },
      "source": [
        "Tokenization in Transformer models is handled differently from tokenization in linear models of the sort we used in Question 1. For Transformer models, we need to use the tokenizer that comes with the model so that we reliably have embedding representations for every token."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hTLIfqEZ8TZd"
      },
      "outputs": [],
      "source": [
        "example_text = \"Bert knows Snuffleupagus\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ak8nyqov8TZd"
      },
      "source": [
        "Here's a basic tokenization step:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hfa5Jrbq8TZd",
        "outputId": "c5304c48-af7a-4360-ab0d-14cebb2a394f"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['bert', 'knows', 's', '##nu', '##ffle', '##up', '##ag', '##us']"
            ]
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "bert_tokenizer.tokenize(example_text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wqu_mZ_w8TZd"
      },
      "source": [
        "Notice that the tokenizer split \"Snuffleupagus\" into a bunch of subword tokens."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s9FIlMYx8TZd"
      },
      "source": [
        "The above use of the tokenizer, where we map from strings to lists of strings, is really for us humans. For modeling, the most important step for tokenization is mapping individual strings to sequences of integer ids. These ids key into the lowest embedding layer of the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nnb4VUX08TZd",
        "outputId": "2488a54e-85e9-406a-95b8-8fa54b6daa5d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[101, 14324, 4282, 1055, 11231, 18142, 6279, 8490, 2271, 102]"
            ]
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "ex_ids = bert_tokenizer.encode(example_text, add_special_tokens=True)\n",
        "\n",
        "ex_ids"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AjHo_KwR8TZd"
      },
      "source": [
        "We can get map these indices back to \"words\" if we want:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6DEQLYOw8TZd",
        "outputId": "477c41ec-a0df-445f-e9d4-15af0f8c76ca"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['[CLS]',\n",
              " 'bert',\n",
              " 'knows',\n",
              " 's',\n",
              " '##nu',\n",
              " '##ffle',\n",
              " '##up',\n",
              " '##ag',\n",
              " '##us',\n",
              " '[SEP]']"
            ]
          },
          "execution_count": 22,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "bert_tokenizer.convert_ids_to_tokens(ex_ids)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2wu4dNOI8TZd"
      },
      "source": [
        "### Background: Representation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Saf19a7R8TZd"
      },
      "source": [
        "Having mapped our string to a list of tokens, we can use the `forward` method of the model to get representations:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6mdDRUmF8TZd"
      },
      "outputs": [],
      "source": [
        "with torch.no_grad():\n",
        "    reps = bert(torch.tensor([ex_ids]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pHclaq4-8TZd"
      },
      "source": [
        "There are a lot of options for which representations to get. With the above call, we got the following:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6KssRwi_8TZd",
        "outputId": "090fbd7f-9e9e-425d-fc05-3d686fbd7289"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "odict_keys(['last_hidden_state', 'pooler_output'])"
            ]
          },
          "execution_count": 24,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "reps.keys()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r8eLlNZz8TZd"
      },
      "source": [
        "The value of `last_hidden_state` hidden state is the sequence of final output states from the model:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NzU68ckn8TZd",
        "outputId": "ec36429c-f46f-4124-aef6-205f9334d06a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([1, 10, 256])"
            ]
          },
          "execution_count": 25,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "reps.last_hidden_state.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ecmCMSe28TZd"
      },
      "source": [
        "This is: 1 example, 10 token representations, each one a 256 dimension vector."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o0B9prAp8TZd"
      },
      "source": [
        "The value of `pooler_output` is a set of currently random parameters sitting on top of the first output hidden state. You can see here that it is a single vector representation per example:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MhLh5Tc18TZd",
        "outputId": "c0953379-a55a-45e0-94e1-5f84ef066004"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([1, 256])"
            ]
          },
          "execution_count": 26,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "reps.pooler_output.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7dhAhnpR8TZd"
      },
      "source": [
        "I often feel unsure of precisely what this model component is. Here we can have a quick look:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NJkYbN5P8TZd",
        "outputId": "13a5f88c-c89a-40f5-e7b1-2c1bff20e2bd"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "BertPooler(\n",
              "  (dense): Linear(in_features=256, out_features=256, bias=True)\n",
              "  (activation): Tanh()\n",
              ")"
            ]
          },
          "execution_count": 27,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "bert.pooler"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NN5tWI6p8TZd"
      },
      "source": [
        "So this is a dense linear layer (a single matrix of weights) with a bias term, and a tanh activation function is applied to the output. We could put a classifier head on top of this if we wanted to, but we might have mixed feelings about being stuck with that tanh step."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XU7qAdlL8TZd"
      },
      "source": [
        "### Background: Masking"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qPj8SaoU8TZd"
      },
      "source": [
        "Where examples from a single batch have different lengths, we need to mask the padded tokens to get the intended results from the model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m_VrBUUO8TZd"
      },
      "source": [
        "For a quick example, here we process our full example from above and print out the first five values:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VUHsMVEP8TZe",
        "outputId": "fa452b74-6a1e-40a5-a0d7-c1158b129286"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([-0.3763, -0.3209,  0.8817,  0.4568, -1.0314])\n"
          ]
        }
      ],
      "source": [
        "with torch.no_grad():\n",
        "    reps = bert(torch.tensor([ex_ids]))\n",
        "    print(reps.last_hidden_state[0][0][: 5])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9ekY_7278TZe"
      },
      "source": [
        "And now we do the same thing, but with masking of the final five positions to illustate:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RE2rluJY8TZe",
        "outputId": "84b9b665-8f8a-4d5d-bbc9-070eb04cbd48"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([-0.1793, -0.8994,  0.9695,  0.9130, -0.7129])\n"
          ]
        }
      ],
      "source": [
        "with torch.no_grad():\n",
        "    # Mask the last 5 tokens:\n",
        "    am = torch.tensor([[1, 1, 1, 1, 1, 0, 0, 0, 0, 0]])\n",
        "    maskreps = bert(torch.tensor([ex_ids]), attention_mask=am)\n",
        "    print(maskreps.last_hidden_state[0][0][: 5])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8yA8eENZ8TZe"
      },
      "source": [
        "### Task 1: Batch tokenization [1 point]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pGOQzznW8TZe"
      },
      "source": [
        "Your task here is to use the `batch_encode_plus` method for `bert_tokenizer` to tokenize a list of strings. You should complete `get_batch_token_ids` according to the specification in the doctring. All these steps can be handled with a single call to `batch_encode_plus`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IErUQrz98TZe"
      },
      "outputs": [],
      "source": [
        "def get_batch_token_ids(batch, tokenizer):\n",
        "    \"\"\"Map `batch` to a tensor of ids. The return\n",
        "    value should meet the following specification:\n",
        "\n",
        "    1. The max length should be 512.\n",
        "    2. Examples longer than the max length should be truncated\n",
        "    3. Examples should be padded to the max length for the batch.\n",
        "    4. The special [CLS] should be added to the start and the special \n",
        "       token [SEP] should be added to the end.\n",
        "    5. The attention mask should be returned\n",
        "    6. The return value of each component should be a tensor.    \n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    batch: list of str\n",
        "    tokenizer: Hugging Face tokenizer\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    dict with at least \"input_ids\" and \"attention_mask\" as keys,\n",
        "    each with Tensor values\n",
        "\n",
        "    \"\"\"\n",
        "    batch_encodings = tokenizer.batch_encode_plus(\n",
        "        batch, \n",
        "        add_special_tokens=True,\n",
        "        padding='longest',\n",
        "        truncation= True,\n",
        "        max_length=512,\n",
        "        return_tensors='pt',\n",
        "        return_attention_mask=True)\n",
        "    \n",
        "    return batch_encodings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RBheQF5I8TZe"
      },
      "source": [
        "Here's a test you can use:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zFbfNsm08TZe"
      },
      "outputs": [],
      "source": [
        "def test_get_batch_token_ids(func):\n",
        "    examples = [\n",
        "        \"Bert knows Snuffleupagus\",\n",
        "        \"ELMo knew Bert.\",\n",
        "        \"Buffalo \" * 520\n",
        "    ]\n",
        "    test_tokenizer = AutoTokenizer.from_pretrained(\"prajjwal1/bert-mini\")\n",
        "    result = func(examples, test_tokenizer)\n",
        "    print(result['attention_mask'])\n",
        "    errcount = 0\n",
        "    if 'attention_mask' not in result:\n",
        "        errcount += 1  \n",
        "        print(f\"Error for `{func.__name__}`: \"\n",
        "              f\"Attention mask was not returned\")\n",
        "    ids = result['input_ids']\n",
        "    if not isinstance(ids, torch.Tensor):\n",
        "        errcount += 1\n",
        "        print(f\"Error for `{func.__name__}`: \"\n",
        "              f\"Return values are not tensors\")\n",
        "    if ids.shape[1] != 512:\n",
        "        errcount += 1\n",
        "        print(f\"Error for `{func.__name__}`: \"\n",
        "              f\"Expected sequence length 512; got {ids.shape[1]}\")\n",
        "    if ids[0][0] != bert_tokenizer.cls_token_id:\n",
        "        errcount += 1\n",
        "        print(f\"Error for `{func.__name__}`: \"\n",
        "              f\"Special tokens were not added\")\n",
        "    if errcount == 0:\n",
        "        print(f\"No errors found for `{func.__name__}`\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qjUSuKRC8TZe",
        "outputId": "d96219dd-481c-4d36-c444-b0b8665cbb15"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
            "        [1, 1, 1,  ..., 0, 0, 0],\n",
            "        [1, 1, 1,  ..., 1, 1, 1]])\n",
            "No errors found for `get_batch_token_ids`\n"
          ]
        }
      ],
      "source": [
        "test_get_batch_token_ids(get_batch_token_ids)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FcofUlN68TZe"
      },
      "source": [
        "### Task 2: Contextual representations [1 point]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ikDfA48n8TZe"
      },
      "source": [
        "This next task is not used directly in fine-tuning, but it should help ensure that you understand how BERT representations are created and how they need to be managed."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "edYatjFV8TZe"
      },
      "source": [
        "Your task is to complete `get_reps` so that, given a dataset (list of strings), it returns a single tensor in which each row is the output hidden state above the [CLS] token for that example. `gets_reps` has a batchsize argument that the user can manage depending on how much available memory they have and how large their model is."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qg71Z2tL8TZe"
      },
      "outputs": [],
      "source": [
        "def get_reps(dataset, model, tokenizer, batchsize=20):\n",
        "    \"\"\"Represent each example in `dataset` with the final hidden state \n",
        "    above the [CLS] token.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    dataset : list of str\n",
        "    model : BertModel\n",
        "    tokenizer : BertTokenizerFast\n",
        "    batchsize : int\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    torch.Tensor with shape `(n_examples, dim)` where `dim` is the\n",
        "    dimensionality of the representations for `model`\n",
        "\n",
        "    \"\"\"\n",
        "    data = []\n",
        "    with torch.no_grad():\n",
        "        # Iterate over `dataset` in batches:\n",
        "        for i in range(0, len(dataset), batchsize):\n",
        "            batch = dataset[i: i+batchsize]\n",
        "\n",
        "            # Encode the batch with `get_batch_token_ids`:\n",
        "            batch_encodings = get_batch_token_ids(batch, tokenizer)\n",
        "\n",
        "            # Get the representations from the model, making\n",
        "            # sure to pay attention to masking:\n",
        "            reps = model(batch_encodings['input_ids'], \n",
        "                         attention_mask=batch_encodings['attention_mask'])\n",
        "            data += reps.last_hidden_state[:,0,:]\n",
        "          \n",
        "        # Return a single tensor:\n",
        "        return torch.stack(data)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F_Q6pbuP8TZe"
      },
      "source": [
        "Quick test:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h7PDTlJq8TZe"
      },
      "outputs": [],
      "source": [
        "def test_get_reps(func):\n",
        "    examples = [\"The cat slept.\", \"The bird chirped.\"] * 20\n",
        "    weights_name = \"prajjwal1/bert-mini\"\n",
        "    test_model = AutoModel.from_pretrained(weights_name)\n",
        "    test_tokenizer = AutoTokenizer.from_pretrained(weights_name)\n",
        "    result = func(examples, test_model, test_tokenizer, batchsize=2)\n",
        "    errcount = 0\n",
        "    if result.shape != (40, 256):\n",
        "        print(f\"Error for `{func.__name__}`: \"\n",
        "              f\"Expected shape {(40, 256)}, got {result.shape}\")\n",
        "    if round(result[0][0].item(), 2) != -0.64:\n",
        "        print(f\"Error for `{func.__name__}`: \"\n",
        "              f\"Representations seem to be incorrect\")\n",
        "    print(f\"No errors found for `{func.__name__}`\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PfDzhLzo8TZe",
        "outputId": "e6015cc1-6be6-49e6-b89e-fa4957451858"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "No errors found for `get_reps`\n"
          ]
        }
      ],
      "source": [
        "test_get_reps(get_reps)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pcmc5dqz8TZe"
      },
      "source": [
        "### Task 3: Fine-tuning module [1 point]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lZM0Uq5a8TZe"
      },
      "source": [
        "We can now put the above together into a basic `nn.Module` that will fine-tune our BERT model. Most of the module is written for you. The pieces you need to implement:\n",
        "\n",
        "1. in the `init` methid, define `self.classifier_layer` using [nn.Sequential](https://pytorch.org/docs/stable/generated/torch.nn.Sequential.html)\n",
        "2. Complete the `forward` method.\n",
        "\n",
        "Precise instructions are provided in the docstrings for the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DI_llzyUJ8hS"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "class BertClassifierModule(nn.Module):\n",
        "    def __init__(self, \n",
        "            n_classes, \n",
        "            hidden_activation, \n",
        "            weights_name=\"prajjwal1/bert-mini\"):\n",
        "        \"\"\"This module loads a Transformer based on  `weights_name`, \n",
        "        puts it in train mode, add a dense layer with activation \n",
        "        function give by `hidden_activation`, and puts a classifier\n",
        "        layer on top of that as the final output. The output of\n",
        "        the dense layer should have the same dimensionality as the\n",
        "        model input.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        n_classes : int\n",
        "            Number of classes for the output layer\n",
        "        hidden_activation : torch activation function\n",
        "            e.g., nn.Tanh()\n",
        "        weights_name : str\n",
        "            Name of pretrained model to load from Hugging Face\n",
        "\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.n_classes = n_classes\n",
        "        self.weights_name = weights_name\n",
        "        self.bert = AutoModel.from_pretrained(self.weights_name)\n",
        "        self.bert.train()\n",
        "        self.hidden_activation = hidden_activation\n",
        "        self.hidden_dim = self.bert.embeddings.word_embeddings.embedding_dim\n",
        "        # Add the new parameters here using `nn.Sequential`. \n",
        "        # We can define this layer as\n",
        "        # \n",
        "        #  h = f(cW1 + b_h)\n",
        "        #  y = hW2 + b_y\n",
        "        #\n",
        "        # where c is the final hidden state above the [CLS] token,\n",
        "        # W1 has dimensionality (self.hidden_dim, self.hidden_dim),\n",
        "        # W2 has dimensionality (self.hidden_dim, self.n_classes), \n",
        "        # and we rely on the PyTorch loss function to add apply a\n",
        "        # softmax to y.  \n",
        "        self.classifier_layer = nn.Sequential(\n",
        "          nn.Linear(self.hidden_dim, self.hidden_dim),\n",
        "          self.hidden_activation,\n",
        "          nn.Linear(self.hidden_dim, self.n_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, indices, mask):\n",
        "        \"\"\"Process `indices` with `mask` by feeding these arguments\n",
        "        to `self.bert` and then feeding the initial hidden state\n",
        "        in `last_hidden_state` to `self.classifier_layer`.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        indices : tensor.LongTensor of shape (n_batch, k)\n",
        "            Indices into the `self.bert` embedding layer. `n_batch` is\n",
        "            the number of examples and `k` is the sequence length for\n",
        "            this batch\n",
        "        mask : tensor.LongTensor of shape (n_batch, d)\n",
        "            Binary vector indicating which values should be masked.\n",
        "            `n_batch` is the number of examples and `k` is the\n",
        "            sequence length for this batch\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        tensor.FloatTensor\n",
        "            Predicted values, shape `(n_batch, self.n_classes)`\n",
        "\n",
        "        \"\"\"\n",
        "        reps = self.bert(indices, attention_mask=mask)\n",
        "        return self.classifier_layer(reps.last_hidden_state[:,0,:])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6KtaRJTY8TZf"
      },
      "outputs": [],
      "source": [
        "bert_module = BertClassifierModule(n_classes=3, hidden_activation=nn.Tanh())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TztYTScg8TZf",
        "outputId": "3076455f-6cf9-4a0b-d577-1bbd8e6d80d9"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[-0.0421,  0.1707, -0.0268],\n",
              "        [-0.1581, -0.1720,  0.0227]], grad_fn=<AddmmBackward0>)"
            ]
          },
          "execution_count": 26,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "ids = get_batch_token_ids(\n",
        "    dynasent_r1['train']['sentence'][: 2],\n",
        "    bert_tokenizer)\n",
        "\n",
        "bert_module(ids['input_ids'], ids['attention_mask'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q3Zh5Pyy8TZf"
      },
      "outputs": [],
      "source": [
        "def test_bert_classifier_module(moduleclass): \n",
        "    expected_out = 5\n",
        "    expected_hidden = 256\n",
        "    expected_activation = nn.ReLU()\n",
        "    mod = moduleclass(expected_out, expected_activation)\n",
        "    errcount = 0\n",
        "\n",
        "    # Basic layer structure:\n",
        "    if not hasattr(mod, \"classifier_layer\") or mod.classifier_layer is None:\n",
        "        errcount += 1\n",
        "        print(f\"Error for `{moduleclass.__name__}`: \"\n",
        "              f\"Missing attribute `classifier_layer`\")\n",
        "        return \n",
        "    for i in range(3):\n",
        "        try:\n",
        "            bert_module.classifier_layer[i]\n",
        "        except IndexError:\n",
        "            errcount += 1\n",
        "            print(f\"Error for `{moduleclass.__name__}`: \"\n",
        "                  f\"`classifier_layer` is not an `nn.Sequential` \"\n",
        "                  f\"and/or does not have the right structure\")\n",
        "    # Correct first layer dimensionality:\n",
        "    result_hidden = mod.classifier_layer[0].out_features\n",
        "    if result_hidden != expected_hidden:\n",
        "        errcount += 1\n",
        "        print(f\"Error for `{moduleclass.__name__}`: \"\n",
        "              f\"Expected `classifier_layer` hidden dim {expected_hidden}, \"\n",
        "              f\"got {result_hidden}\") \n",
        "    # Correct activation:\n",
        "    result_activation = mod.classifier_layer[1].__class__.__name__\n",
        "    if result_activation != expected_activation.__class__.__name__:\n",
        "        errcount += 1\n",
        "        print(f\"Error for `{moduleclass.__name__}`: \"\n",
        "              f\"Incorrect hidden activation\")\n",
        "    # Correct output dimensionality:\n",
        "    result_out = mod.classifier_layer[2].out_features\n",
        "    if result_out != expected_out:\n",
        "        errcount += 1\n",
        "        print(f\"Error for `{moduleclass.__name__}`: \"\n",
        "              f\"Expected `classifier_layer` out dim {expected_out}, \"\n",
        "              f\"got {result_out}\")\n",
        "    # forward method:\n",
        "    ids = get_batch_token_ids([\"A B C\", \"A B\"], bert_tokenizer)\n",
        "    result = mod(ids['input_ids'], ids['attention_mask'])\n",
        "    if result.shape != (2, 5):\n",
        "        errcount += 1\n",
        "        print(f\"Error for `{moduleclass.__name__}`: \"\n",
        "              f\"Expected output shape {(2, 5)}, got {result.shape}\")\n",
        "    if errcount == 0:\n",
        "        print(f\"No errors found for `{moduleclass.__name__}`\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MNsxL2Uu8TZh",
        "outputId": "7e9351fc-4692-4ac3-c202-1cf5d97005b7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "No errors found for `BertClassifierModule`\n"
          ]
        }
      ],
      "source": [
        "test_bert_classifier_module(BertClassifierModule)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "coCrevO88TZh"
      },
      "source": [
        "### Optional use: Classifier interface"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4OZHnalC8TZh"
      },
      "source": [
        "The above module doesn't have functionality for processing data and fitting models. Our course code includes some general purpose code for adding these features. Here is an example that should work well with the module you wrote above. For more details on the design of these interfaces, see [tutorial_pytorch_models.ipynb](tutorial_pytorch_models.ipynb)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rfD8yC4AJ8hT"
      },
      "outputs": [],
      "source": [
        "from torch_shallow_neural_classifier import TorchShallowNeuralClassifier\n",
        "\n",
        "class BertClassifier(TorchShallowNeuralClassifier):\n",
        "    def __init__(self, weights_name, *args, **kwargs):\n",
        "        self.weights_name = weights_name\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(self.weights_name)\n",
        "        super().__init__(*args, **kwargs)\n",
        "        self.params += ['weights_name']\n",
        "\n",
        "    # define the neural network architecture\n",
        "    def build_graph(self):\n",
        "        return BertClassifierModule(\n",
        "            self.n_classes_, self.hidden_activation, self.weights_name)\n",
        "\n",
        "    def build_dataset(self, X, y=None):\n",
        "        data = get_batch_token_ids(X, self.tokenizer)\n",
        "        if y is None:\n",
        "            dataset = torch.utils.data.TensorDataset(\n",
        "                data['input_ids'], data['attention_mask'])\n",
        "        else:\n",
        "            self.classes_ = sorted(set(y))\n",
        "            self.n_classes_ = len(self.classes_)\n",
        "            class2index = dict(zip(self.classes_, range(self.n_classes_)))\n",
        "            y = [class2index[label] for label in y]\n",
        "            y = torch.tensor(y)\n",
        "            dataset = torch.utils.data.TensorDataset(\n",
        "                data['input_ids'], data['attention_mask'], y)\n",
        "        return dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yqgdaFRi8TZh"
      },
      "source": [
        "And here is a training run that should do pretty well for our problem. \n",
        "\n",
        "__Note__: This step should not be run on CPU machines. On Google Colab with a GPU, it will likely take about an hour."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "re8lzSepJ8hT"
      },
      "outputs": [],
      "source": [
        "bert_finetune = BertClassifier(\n",
        "    weights_name=\"prajjwal1/bert-mini\",\n",
        "    hidden_activation=nn.ReLU(),\n",
        "    eta=0.00005,          # Low learning rate for effective fine-tuning.\n",
        "    batch_size=8,         # Small batches to avoid memory overload.\n",
        "    gradient_accumulation_steps=4,  # Increase the effective batch size to 32.\n",
        "    early_stopping=True,  # Early-stopping\n",
        "    n_iter_no_change=5)   # params."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wlvq0G-PJ8hT",
        "outputId": "52bd5dda-82be-463e-ad71-145dd910e4a2"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Stopping after epoch 11. Validation score did not improve by tol=1e-05 for more than 5 epochs. Final error is 480.18226193473674"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CPU times: user 1h 16min 48s, sys: 18.4 s, total: 1h 17min 6s\n",
            "Wall time: 1h 17min 46s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "\n",
        "_ = bert_finetune.fit(\n",
        "    dynasent_r1['train']['sentence'],\n",
        "    dynasent_r1['train']['gold_label'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WwwiIuKqJ8hT"
      },
      "outputs": [],
      "source": [
        "preds = bert_finetune.predict(sst['validation']['sentence'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q3Af7RAOJ8hT",
        "outputId": "1254a69b-8f7f-492b-945a-8587375489a1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "    negative      0.605     0.561     0.582       428\n",
            "     neutral      0.320     0.432     0.368       229\n",
            "    positive      0.696     0.619     0.656       444\n",
            "\n",
            "    accuracy                          0.558      1101\n",
            "   macro avg      0.540     0.537     0.535      1101\n",
            "weighted avg      0.582     0.558     0.567      1101\n",
            "\n"
          ]
        }
      ],
      "source": [
        "print(classification_report(sst['validation']['gold_label'], preds, digits=3))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YOblLwl3J8hU"
      },
      "outputs": [],
      "source": [
        "preds = bert_finetune.predict(dynasent_r1['validation']['sentence'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ndDbPLY1b7lj",
        "outputId": "5f003b49-3899-420a-be4f-5011e006cd8e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "    negative      0.766     0.588     0.665      1200\n",
            "     neutral      0.625     0.869     0.727      1200\n",
            "    positive      0.778     0.656     0.712      1200\n",
            "\n",
            "    accuracy                          0.704      3600\n",
            "   macro avg      0.723     0.704     0.701      3600\n",
            "weighted avg      0.723     0.704     0.701      3600\n",
            "\n"
          ]
        }
      ],
      "source": [
        "print(classification_report(dynasent_r1['validation']['gold_label'], preds, digits=3))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bh0Siu688TZi"
      },
      "source": [
        "## Question 3: Your original system [3 points]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PULUblVj8TZi"
      },
      "source": [
        "Your task is to develop an original ternary sentiment classifier model. There are many options. The only rule:\n",
        "\n",
        "__You cannot make any use of the test sets for DynaSent-R1, DynaSent-R2, or SST-3, at any time during the course of development.__\n",
        "\n",
        "The integrity of the bakeoff depends on this rule being followed.\n",
        "\n",
        "It's fine to use the dev sets for system development – indeed, we encourage this.\n",
        "\n",
        "For system development, here are some relatively manageable ideas that you might try:\n",
        "\n",
        "* Different pretrained models. There are many models available on the [Hugging Face models hub](https://huggingface.co/models) that will be drop-in replacements for BERT-mini as we used it above.\n",
        "\n",
        "* Different fine-tuning regimes. We used the [CLS] token above. This doesn't make especially good use of the output states of the models. Pooling across these representtions (with sum, average, etc.) is likely to be better.\n",
        "\n",
        "* Different training regimes. You have three train sets at your disposal, and there may be other sentiment datasets that could contribute to making your system more robust in new domains.\n",
        "\n",
        "* Entirely different approaches. There is no requirement that you make use of any of the concepts from the homework questions in constructing your original system. Anything goes as long as you follow the one rule given above in bold.\n",
        "\n",
        "We want to emphasize that this needs to be an original system. It doesn't suffice to download code from the Web, retrain, and submit. You can build on others' code, but you have to do something new and meaningful with it. See the course website for additional guidance on how original systems will be evaluated.\n",
        "\n",
        "In the cell below, please provide a brief technical description of your original system, so that the teaching team can gain an understanding of what it does. This will help us to understand your code and analyze all the submissions to identify patterns and strategies."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Oeehap178TZi"
      },
      "outputs": [],
      "source": [
        "# PLEASE MAKE SURE TO INCLUDE THE FOLLOWING BETWEEN THE START AND STOP COMMENTS:\n",
        "#   1) Textual description of your system.\n",
        "#   2) The code for your original system.\n",
        "# PLEASE MAKE SURE NOT TO DELETE OR EDIT THE START AND STOP COMMENTS\n",
        "\n",
        "# START COMMENT: Enter your system description in this cell.\n",
        "\n",
        "# STOP COMMENT: Please do not remove this comment."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kkWqJMzzkEDZ"
      },
      "source": [
        "\n",
        "\n",
        "*   try a new-pretrained model\n",
        "*   max pooling / sum pooling the final hidden states\n",
        "*   deeper classifier \n",
        "    * with GridSearch for parameters\n",
        "* \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mzN-YkcKXXDi"
      },
      "outputs": [],
      "source": [
        "import transformers\n",
        "from transformers import AutoModel, AutoTokenizer\n",
        "import torch.nn as nn\n",
        "from torch_shallow_neural_classifier import TorchShallowNeuralClassifier\n",
        "from sklearn.metrics import classification_report"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o69h6Mv-Thig"
      },
      "outputs": [],
      "source": [
        "def get_batch_token_ids(batch, tokenizer):\n",
        "\n",
        "    batch_encodings = tokenizer.batch_encode_plus(\n",
        "        batch, \n",
        "        add_special_tokens=True,\n",
        "        padding='longest',\n",
        "        truncation= True,\n",
        "        max_length=512,\n",
        "        return_tensors='pt',\n",
        "        return_attention_mask=True)\n",
        "    \n",
        "    return batch_encodings\n",
        "\n",
        "class BertClassifierModule(nn.Module):\n",
        "    def __init__(self, \n",
        "            n_classes, \n",
        "            hidden_activation, \n",
        "            weights_name=\"prajjwal1/bert-mini\"):\n",
        "\n",
        "        super().__init__()\n",
        "        self.n_classes = n_classes\n",
        "        self.weights_name = weights_name\n",
        "        self.bert = AutoModel.from_pretrained(self.weights_name)\n",
        "        self.bert.train()\n",
        "        self.hidden_activation = hidden_activation\n",
        "        self.hidden_dim = self.bert.embeddings.word_embeddings.embedding_dim\n",
        "        self.classifier_layer = nn.Sequential(\n",
        "          nn.Linear(self.hidden_dim, self.hidden_dim),\n",
        "          self.hidden_activation,\n",
        "          nn.Linear(self.hidden_dim, self.n_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, indices, mask):\n",
        "\n",
        "        reps = self.bert(indices, attention_mask=mask)\n",
        "        max_pooled_output, _ = torch.max(reps.last_hidden_state, dim=1)\n",
        "        return self.classifier_layer(max_pooled_output)\n",
        "\n",
        "class BertClassifier(TorchShallowNeuralClassifier):\n",
        "    def __init__(self, weights_name, *args, **kwargs):\n",
        "        self.weights_name = weights_name\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(self.weights_name)\n",
        "        super().__init__(*args, **kwargs)\n",
        "        self.params += ['weights_name']\n",
        "\n",
        "    # define the neural network architecture\n",
        "    def build_graph(self):\n",
        "        return BertClassifierModule(\n",
        "            self.n_classes_, self.hidden_activation, self.weights_name)\n",
        "\n",
        "    def build_dataset(self, X, y=None):\n",
        "        data = get_batch_token_ids(X, self.tokenizer)\n",
        "        if y is None:\n",
        "            dataset = torch.utils.data.TensorDataset(\n",
        "                data['input_ids'], data['attention_mask'])\n",
        "        else:\n",
        "            self.classes_ = sorted(set(y))\n",
        "            self.n_classes_ = len(self.classes_)\n",
        "            class2index = dict(zip(self.classes_, range(self.n_classes_)))\n",
        "            y = [class2index[label] for label in y]\n",
        "            y = torch.tensor(y)\n",
        "            dataset = torch.utils.data.TensorDataset(\n",
        "                data['input_ids'], data['attention_mask'], y)\n",
        "        return dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wyUqaRPRX0iC"
      },
      "outputs": [],
      "source": [
        "bert_finetune = BertClassifier(\n",
        "    weights_name=\"prajjwal1/bert-mini\",\n",
        "    hidden_activation=nn.ReLU(),\n",
        "    eta=0.00005,          # Low learning rate for effective fine-tuning.\n",
        "    batch_size=8,         # Small batches to avoid memory overload.\n",
        "    gradient_accumulation_steps=4,  # Increase the effective batch size to 32.\n",
        "    early_stopping=True,  # Early-stopping\n",
        "    n_iter_no_change=5)   # params."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3eyKqIkHWLe2",
        "outputId": "373fa921-179c-469c-a21d-1f28e2aa4ffc"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at prajjwal1/bert-mini were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Stopping after epoch 11. Validation score did not improve by tol=1e-05 for more than 5 epochs. Final error is 445.7139027687663"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CPU times: user 1h 21min 24s, sys: 18.8 s, total: 1h 21min 43s\n",
            "Wall time: 1h 21min 58s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "\n",
        "_ = bert_finetune.fit(\n",
        "    dynasent_r1['train']['sentence'],\n",
        "    dynasent_r1['train']['gold_label'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M1omdvnatB1B",
        "outputId": "c31b181e-31b9-4755-b11a-e294d3dd4fd5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "    negative      0.594     0.605     0.600       428\n",
            "     neutral      0.314     0.371     0.340       229\n",
            "    positive      0.670     0.595     0.630       444\n",
            "\n",
            "    accuracy                          0.552      1101\n",
            "   macro avg      0.526     0.524     0.523      1101\n",
            "weighted avg      0.566     0.552     0.558      1101\n",
            "\n"
          ]
        }
      ],
      "source": [
        "preds = bert_finetune.predict(sst['validation']['sentence'])\n",
        "print(classification_report(sst['validation']['gold_label'], preds, digits=3))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L-gKfOVftVth",
        "outputId": "d9017037-eb09-474d-fd59-e9adc6a2e5fb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "    negative      0.781     0.598     0.677      1200\n",
            "     neutral      0.658     0.839     0.737      1200\n",
            "    positive      0.734     0.704     0.719      1200\n",
            "\n",
            "    accuracy                          0.714      3600\n",
            "   macro avg      0.724     0.714     0.711      3600\n",
            "weighted avg      0.724     0.714     0.711      3600\n",
            "\n"
          ]
        }
      ],
      "source": [
        "preds = bert_finetune.predict(dynasent_r1['validation']['sentence'])\n",
        "print(classification_report(dynasent_r1['validation']['gold_label'], preds, digits=3))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oWfwdyThZolf"
      },
      "outputs": [],
      "source": [
        "from datasets import concatenate_datasets, DatasetDict\n",
        "combined_data = DatasetDict({'train': concatenate_datasets([dynasent_r1['train'], dynasent_r2['train'], sst['train']]), \n",
        "                             'validation': concatenate_datasets([dynasent_r1['validation'], dynasent_r2['validation'], sst['validation']]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Oqf35aRR7Hbc"
      },
      "outputs": [],
      "source": [
        "preds = bert_finetune.predict(combined_data['validation']['sentence'])\n",
        "print(classification_report(combined_data['validation']['gold_label'], preds, digits=3))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hh-3JIrQc8eg"
      },
      "outputs": [],
      "source": [
        "if CUDA:\n",
        "  model = model.to('cuda')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IwfmMy0njKtw"
      },
      "outputs": [],
      "source": [
        "class TorchDeeperNeuralClassifier(TorchShallowNeuralClassifier):\n",
        "    def __init__(self, hidden_dim1=50, hidden_dim2=50, **base_kwargs):\n",
        "        super().__init__(**base_kwargs)\n",
        "        self.hidden_dim1 = hidden_dim1\n",
        "        self.hidden_dim2 = hidden_dim2\n",
        "        # Good to remove this to avoid confusion:\n",
        "        self.params.remove(\"hidden_dim\")\n",
        "        # Add the new parameters to support model_selection using them:\n",
        "        self.params += [\"hidden_dim1\", \"hidden_dim2\"]\n",
        "\n",
        "    def build_graph(self):\n",
        "        return nn.Sequential(\n",
        "            nn.Linear(self.input_dim, self.hidden_dim1),\n",
        "            self.hidden_activation,\n",
        "            nn.Linear(self.hidden_dim1, self.hidden_dim2),\n",
        "            self.hidden_activation,\n",
        "            nn.Linear(self.hidden_dim2, self.n_classes_))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VX6pipFYjaKL"
      },
      "outputs": [],
      "source": [
        "xval = GridSearchCV(\n",
        "    TorchDeeperNeuralClassifier(),\n",
        "    param_grid={\n",
        "        'hidden_dim1': [5, 10],\n",
        "        'hidden_dim2': [5, 10]})\n",
        "\n",
        "_ = xval.fit(X_cls_train, y_cls_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NzWzEgyajc-C"
      },
      "outputs": [],
      "source": [
        "xval.best_estimator_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i3dJLNRDkCdu"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wDVcSFJX8TZi"
      },
      "source": [
        "## Question 4: Bakeoff entry [1 point]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x8TiOoa78TZi"
      },
      "source": [
        "The bakeoff dataset is available at \n",
        "\n",
        "https://web.stanford.edu/class/cs224u/data/cs224u-sentiment-test-unlabeled.csv\n",
        "\n",
        "This code should grab it for you and put it in `data/sentiment` if you are working in the cloud:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Sy-Brh8E8TZi"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "if not os.path.exists(os.path.join(\"data\", \"sentiment\", \"cs224u-sentiment-test-unlabeled.csv\")):\n",
        "    !mkdir -p data/sentiment\n",
        "    !wget https://web.stanford.edu/class/cs224u/data/cs224u-sentiment-test-unlabeled.csv -P data/sentiment/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hi0RGDx_8TZi"
      },
      "source": [
        "If the above fails, you can just download the file and place it in `data/sentiment`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L2BGErFg8TZi"
      },
      "source": [
        "Once you have the file, you can load it to a `pd.DataFrame`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hGhpnZ5A8TZi"
      },
      "outputs": [],
      "source": [
        "bakeoff_df = pd.read_csv(\n",
        "    os.path.join(\"data\", \"sentiment\", \"cs224u-sentiment-test-unlabeled.csv\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rEj3LGxP8TZi"
      },
      "outputs": [],
      "source": [
        "bakeoff_df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fnyfnPnT8TZi"
      },
      "source": [
        "To enter the bakeoff, you simply need to use your original system t:\n",
        "\n",
        "1. Add a column named 'prediction' to `cs224u-sentiment-test-unlabeled.csv` with your model predictions (which are strings in {`positive`, `negative`, `neutral`}). The existing columns should remain.\n",
        "\n",
        "2. Save the file as `cs224u-sentiment-bakeoff-entry.csv`.\n",
        "\n",
        "Submit the following files to Gradescope:\n",
        "\n",
        "* `hw_sentiment.ipynb` (this notebook)\n",
        "* `cs224u-sentiment-bakeoff-entry.csv` (bake-off output)\n",
        "\n",
        "Please make sure you use these filenames. The autograder looks for files with these names.\n",
        "\n",
        "You are not permitted to do any tuning of your system based on what you see in our bakeoff prediction file – you should not study that file in anyway, beyond perhaps checking that it contains what you expected it to contain. The upload function will do some additional checking to ensure that your file is well-formed.\n",
        "\n",
        "People who enter will receive the additional homework point, and people whose systems achieve the top score will receive an additional 0.5 points. We will test the top-performing systems ourselves, and only systems for which we can reproduce the reported results will win the extra 0.5 points.\n",
        "\n",
        "Late entries will be accepted, but they cannot earn the extra 0.5 points."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "machine_shape": "hm",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "050c57c2ad7842f2815cc6e697a1b86b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "05bd8598144846d183e71a82a67a6bc0": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_11beceea418f426d86f9f28ef6f4f8ca",
              "IPY_MODEL_09481ac101eb40e7b3db001e227ed04f",
              "IPY_MODEL_5d01709b419244c68d6ce1e6ff941cf2"
            ],
            "layout": "IPY_MODEL_9a9c4a540ae344689232f6ae3617d2db"
          }
        },
        "06626014e1884b2cada8fc05c65babba": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_d70b0e902b374ebe976b8eb6e2455b9f",
              "IPY_MODEL_1cea5f29c7c849b8b748596ef92f3b7f",
              "IPY_MODEL_fb05390f8941496f9778f71064f5a23f"
            ],
            "layout": "IPY_MODEL_98a1f6e511b6431a912ed1559184de3d"
          }
        },
        "09481ac101eb40e7b3db001e227ed04f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ebc1ce58e78948bead4a2a2a4b2bb5cc",
            "max": 286,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_cc3619b8705345688badafe8306c887f",
            "value": 286
          }
        },
        "10f02f9aeed54f66b265d1fd40bebfd1": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "11beceea418f426d86f9f28ef6f4f8ca": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f3f787ef507340559ae03a34422b3f61",
            "placeholder": "​",
            "style": "IPY_MODEL_6cefcd18445444c2a117e185410f4219",
            "value": "Downloading (…)lve/main/config.json: 100%"
          }
        },
        "18045b61283c4b21bb9ec779b499a8ad": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "18e56f5dcb5047959fb91433722cf982": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f71c5bc693f1439c9e9097b461983298",
            "max": 45106985,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_5f3eea9c9f50414a90012cf4a0bd941a",
            "value": 45106985
          }
        },
        "1cea5f29c7c849b8b748596ef92f3b7f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d53950f033e9462c9bed528d026485ef",
            "max": 231508,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_18045b61283c4b21bb9ec779b499a8ad",
            "value": 231508
          }
        },
        "29e3f35345b24155b7a9b3b275e97268": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "321442faba684d589631eb8cf062ae2b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5d01709b419244c68d6ce1e6ff941cf2": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_934787727a5a43f2b0d8c2a05f61dbd6",
            "placeholder": "​",
            "style": "IPY_MODEL_321442faba684d589631eb8cf062ae2b",
            "value": " 286/286 [00:00&lt;00:00, 11.1kB/s]"
          }
        },
        "5f3eea9c9f50414a90012cf4a0bd941a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "609ea1432a9f4a3798b6bdc4a876e7cb": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6cefcd18445444c2a117e185410f4219": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "734131b2a4a64449bacf7720f96f5707": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a446bccf10fd4d3fb0fd479acc31edb2",
            "placeholder": "​",
            "style": "IPY_MODEL_609ea1432a9f4a3798b6bdc4a876e7cb",
            "value": " 45.1M/45.1M [00:00&lt;00:00, 107MB/s]"
          }
        },
        "934787727a5a43f2b0d8c2a05f61dbd6": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "93a8b8e390454f079a917a7b4c2a19f4": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "98a1f6e511b6431a912ed1559184de3d": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9a9c4a540ae344689232f6ae3617d2db": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a446bccf10fd4d3fb0fd479acc31edb2": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a987ea5cb2af4023a2ee907a188f4028": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ec80f21750f64f51868fea4c66c88a69",
            "placeholder": "​",
            "style": "IPY_MODEL_93a8b8e390454f079a917a7b4c2a19f4",
            "value": "Downloading pytorch_model.bin: 100%"
          }
        },
        "c193a071ae9649b7bd37cebcdc177531": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_a987ea5cb2af4023a2ee907a188f4028",
              "IPY_MODEL_18e56f5dcb5047959fb91433722cf982",
              "IPY_MODEL_734131b2a4a64449bacf7720f96f5707"
            ],
            "layout": "IPY_MODEL_29e3f35345b24155b7a9b3b275e97268"
          }
        },
        "c7e64bab54c14e4498a3bb118a9853e9": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "cc3619b8705345688badafe8306c887f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "d53950f033e9462c9bed528d026485ef": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d70b0e902b374ebe976b8eb6e2455b9f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_10f02f9aeed54f66b265d1fd40bebfd1",
            "placeholder": "​",
            "style": "IPY_MODEL_c7e64bab54c14e4498a3bb118a9853e9",
            "value": "Downloading (…)solve/main/vocab.txt: 100%"
          }
        },
        "ebc1ce58e78948bead4a2a2a4b2bb5cc": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ec6f2b4cdea84002b110c481c7bcf926": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ec80f21750f64f51868fea4c66c88a69": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f3f787ef507340559ae03a34422b3f61": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f71c5bc693f1439c9e9097b461983298": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fb05390f8941496f9778f71064f5a23f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ec6f2b4cdea84002b110c481c7bcf926",
            "placeholder": "​",
            "style": "IPY_MODEL_050c57c2ad7842f2815cc6e697a1b86b",
            "value": " 232k/232k [00:00&lt;00:00, 1.63MB/s]"
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}